{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2d9b7-5ab8-4f26-b3b0-bd0c4786cf39",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Ask the Graph\n",
    "# Notebook 1: Prep Unstructured Data\n",
    "\n",
    "In this notebook we prepare unstructured data, a set of press releases about organizations. This data complements the structured organization data we prepared in notebook 0. \n",
    "\n",
    "You DO NOT need to run this notebook. The notebook produces output that is already prepared for you and available in a public bucket. But you may wish to review how we prepared that data. In that case, follow along with the logic below.\n",
    "\n",
    "Here is the input, sourced from s3://aws-neptune-customer-samples/tmls2024/source/. We download a local copy to the source folder:\n",
    "\n",
    "- comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl: Press releases, including title, keywords, and text. This file also contains entities and events extracted from press releases using Amazon Comprehend.\n",
    "\n",
    "We produce the following output locally:\n",
    "- graphdata/lpg - CSV files of embeddings and extracted entities to bulk-load as LPG data to Neptune. \n",
    "- graphdata/rdf - Turtle files to bulk-load to Neptune as RDF data. (TODO - is this needed?)\n",
    "- documents - Press releases as .txt files \n",
    "- chunks - Chunks of press releases as .txt files\n",
    "- summaries - Summaries of press releases as .txt files\n",
    "\n",
    "That same data is available in:\n",
    "s3://aws-neptune-customer-samples/tmls2024/graphdata/\n",
    "s3://aws-neptune-customer-samples/tmls2024/documents/\n",
    "s3://aws-neptune-customer-samples/tmls2024/chunks/\n",
    "s3://aws-neptune-customer-samples/tmls2024/summaries/\n",
    "\n",
    "Specific files that we produce are the following. LPG files in graphdata/lpg are:\n",
    "\n",
    "- documents.csv - Press release documents and their summary as an embedding.\n",
    "- summaries.csv - References the documents from documents.csv but adding their embedding from summary.\n",
    "- extractions.csv - Entities and events extracted from press releases.\n",
    "- extraction_rels.csv - Links between documents and their extracted entities and events. \n",
    "- resolved_entities.csv - Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "- resolution_links.csv -  Resolution matches that can be linked from extracted node to structured node.\n",
    "- chunks.csv - Document chunks and their embeddings.\n",
    "- chunk2doc.csv -Link chunks to docs.\n",
    "- entity_embeddings.csv - Embeddings of entities for searchability\n",
    "\n",
    "RDF in graphdata/rdf are:\n",
    "- documents.ttl - Press release documents.\n",
    "- extractions.ttl - Entities and events extracted from press releases.\n",
    "- resolved_entities.ttl -Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "\n",
    "TODO mention embeddings are kept in OpenSearch for RDF..\n",
    "\n",
    "These files are also maintained in:\n",
    "\n",
    "- s3://aws-neptune-customer-samples/tmls2024/graphdata/\n",
    "- s3://aws-neptune-customer-samples/tmls2024/documents/\n",
    "- s3://aws-neptune-customer-samples/tmls2024/chunks/\n",
    "- s3://aws-neptune-customer-samples/tmls2024/summaries/\n",
    "\n",
    "TODO - data model, including what we've built so far\n",
    "\n",
    "In the next notebook, we load prepared data into Neptune for query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0abb6f4-313a-45cd-95c7-02a7dbbba2f6",
   "metadata": {},
   "source": [
    "## Install a few dependencies\n",
    "We use Langchain very lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f1990855-7e68-4173-bc4e-f0bfa8d2844d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters langchain-community unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18867a0",
   "metadata": {},
   "source": [
    "## Get source data and create output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267563be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://aws-neptune-customer-samples-us-east-1/tmls2024/source/ source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db952cc7-326d-4264-ada2-60d88eb676fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "mkdir -p graphdata graphdata/rdf graphdata/lpg summaries chunks documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf1f3e-7db2-4c0f-82f9-cb71d1c1fa81",
   "metadata": {},
   "source": [
    "## Build press release documents\n",
    "\n",
    "We have a set of press releases. These documents contain useful information that we would like to link to our base organization KG. \n",
    "\n",
    "In this set we build those documents.\n",
    "\n",
    "The result is a CSV file written to the *graphdata* folder:\n",
    "\n",
    "- lpg/documents.csv \n",
    "- rdf/documents.ttl\n",
    "\n",
    "Also *.txt* files are written to the *documents* folder. The name of each file is *docid*.txt, where *docid* is the vertex ID of the document in the graph.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b93e8978-41ac-439d-ae38-96cafc37f00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_document_uri(docid):\n",
    "    return helpers.make_uri(f\"Document/{docid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc326317-220f-41c3-9040-facad75cbea2",
   "metadata": {},
   "source": [
    "## The RAG-prep part: Summarize docs Chunk, embed, and extract files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0887b1e4-bbab-4a56-8330-2e95940f239e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, json\n",
    "import helpers\n",
    "from rdflib import Graph, Literal, RDF, RDFS, URIRef, XSD, OWL, BNode, DC, SKOS\n",
    "\n",
    "prdocs = []\n",
    "\n",
    "# Open the JSONL file that contains the documents plus the Comprehend extraction. \n",
    "jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "for index, row in jsonObj.iterrows():\n",
    "        \n",
    "    # extract metadata about current press release\n",
    "    metadata=row['metadata']\n",
    "    m_keywords=metadata['keywords']\n",
    "    m_title=metadata['title']\n",
    "    m_doc=metadata['document_id']\n",
    "\n",
    "    # write text to a file for chunking/embedding later\n",
    "    with open(f\"documents/{m_doc}.txt\", \"w\") as f:\n",
    "        f.write(row['raw_text'])\n",
    "    \n",
    "    prdocs.append([make_document_uri(m_doc), \"Document\", m_doc, m_title, m_keywords])\n",
    "\n",
    "# write docs to CSV for LPG\n",
    "with open('graphdata/lpg/documents.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"docuuid\", \"title\", \"keywords\"])\n",
    "    for p in prdocs:\n",
    "        writer.writerow(p)\n",
    "\n",
    "# write docs for RDF\n",
    "rdf_file = helpers.rdf_open()\n",
    "for p in prdocs:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), RDF.type, helpers.make_uri(\"Document\"))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), DC.title, Literal(p[3]))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), helpers.make_uri(\"uuid\"), Literal(p[2]))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), helpers.make_uri(\"keywords\"), Literal(p[4]))\n",
    "    \n",
    "        \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/documents.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efa70d-9690-4f45-8d83-f01f8cdfd46b",
   "metadata": {},
   "source": [
    "## Build Comprehend extraction results\n",
    "\n",
    "We ran Amazon Comprehend to extract entities and events from the press releases. \n",
    "\n",
    "In this step, we build those extractions and link them to documents.\n",
    "\n",
    "Specific files that we produce are the followin:\n",
    "\n",
    "- extracted_entities.csv\n",
    "- extracted_events.csv \n",
    "- extraction.ttl - Entities and events extracted from press releases.\n",
    "\n",
    "For more on this approach, see blog post https://aws.amazon.com/blogs/database/building-a-knowledge-graph-in-amazon-neptune-using-amazon-comprehend-events/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54911735-f04b-421d-a08b-f8ddb5396e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will filter out names referring to each entity with less than 0.95 group certainty.\n",
    "# You can change this threshold to be lower if you are tolerant of less certain values in your data set.\n",
    "groupThreshold = 0.95\n",
    "\n",
    "entities=[]\n",
    "events=[]\n",
    "ee_rels=[]\n",
    "de_rels=[]\n",
    "distinct_roles={}\n",
    "\n",
    "def strip_x_id(rawid):\n",
    "    return \"_\".join(rawid.split()).lower() # replace all whitespace with underscores\n",
    "\n",
    "# Open the JSONL file that contains the documents plus the Comprehend extraction. \n",
    "jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "for index, row in jsonObj.iterrows():\n",
    "        \n",
    "    # extract metadata about current press release\n",
    "    metadata=row['metadata']\n",
    "    m_doc=metadata['document_id']\n",
    "    \n",
    "    # Comprehend Events references entities it refers to by index, so we need to retain the ordered list of entities\n",
    "    # within the document\n",
    "    annotations = row['annotations']\n",
    "    for entity in annotations[\"Entities\"]:\n",
    "        primary_name = entity[\"Mentions\"][0][\"Text\"]\n",
    "        entity_type =entity[\"Mentions\"][0][\"Type\"]\n",
    "        entity_local=strip_x_id(f\"{entity_type}_{primary_name}\")\n",
    "        entity_id= helpers.make_uri(f\"ExtractedEntity/{entity_local}\")\n",
    "        names=[]\n",
    "        for mention in entity[\"Mentions\"]:\n",
    "            if (mention[\"GroupScore\"] >= groupThreshold):\n",
    "                if not(mention[\"Text\"] in names):\n",
    "                        names.append(mention[\"Text\"])\n",
    "        entities.append({\n",
    "            '~id': entity_id,\n",
    "            'local_id': entity_local,\n",
    "            '~label': 'ExtractedEntity',\n",
    "            'label': primary_name,\n",
    "            'labels': names,\n",
    "            'type': entity_type\n",
    "        })\n",
    "        \n",
    "    for event in annotations[\"Events\"]:\n",
    "        primary_name=event[\"Triggers\"][0][\"Text\"]\n",
    "        event_type=event[\"Type\"]\n",
    "        offset_for_id=str(event[\"Triggers\"][0][\"BeginOffset\"])\n",
    "        event_local=strip_x_id(f\"{m_doc}_{event_type}_{primary_name}{offset_for_id}\")\n",
    "        event_id=helpers.make_uri(f\"ExtractedEvent/{event_local}\")\n",
    "        names=[]\n",
    "        for trigger in event[\"Triggers\"]:\n",
    "            if not(trigger[\"Text\"] in names):\n",
    "                names.append(trigger[\"Text\"])\n",
    "\n",
    "        events.append({\n",
    "            '~id': event_id,\n",
    "            'local_id': event_local,\n",
    "            '~label': 'ExtractedEvent',\n",
    "            'label': primary_name,\n",
    "            'labels': names,\n",
    "            'type': event_type\n",
    "        })\n",
    "\n",
    "        # add edges between the event node and the entity node, \n",
    "        # annotated with a label describing the Comprehend Event role assigned to the entity in the event.\n",
    "        for argument in event[\"Arguments\"]:\n",
    "            from_id=event_id\n",
    "            from_local=event_local\n",
    "            to_ent=entities[argument[\"EntityIndex\"]]\n",
    "            to_id=to_ent['~id']\n",
    "            to_local=to_ent['local_id']\n",
    "            edge_type=argument[\"Role\"]\n",
    "            ee_local=strip_x_id(f\"{from_local}_{to_local}_{edge_type}\")\n",
    "            ee_edge_id=helpers.make_uri(f\"ExtractedEventToEntity/{ee_local}\")\n",
    "            role= argument[\"Role\"]\n",
    "            role_uri=helpers.make_uri(f\"ExtractedRole/{role}\")\n",
    "            distinct_roles[role_uri]=role\n",
    "            ee_rels.append({\n",
    "                '~id': ee_edge_id,\n",
    "                '~label': \"eventHasEntity\",\n",
    "                'type': event[\"Type\"],\n",
    "                '~from': from_id,\n",
    "                '~to': to_id,\n",
    "                \"role\":  role\n",
    "            })\n",
    "        \n",
    "        # add an edge between the document and the event nodes\n",
    "        document_id = make_document_uri(m_doc)\n",
    "        de_local=strip_x_id(f\"{m_doc}_{event_local}\")\n",
    "        de_edge_id=helpers.make_uri(f\"DocumentToExtractedEvent/{de_local}\")\n",
    "        de_rels.append({\n",
    "            '~id': de_edge_id,\n",
    "            '~label': \"documentHasEvent\",\n",
    "            '~from': document_id,\n",
    "            '~to': event_id,\n",
    "            'role': \"\" \n",
    "        })\n",
    "        \n",
    "\n",
    "# write docs to CSV for LPG\n",
    "with open('graphdata/lpg/extractions.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"label\", \"labels\", \"type\"])\n",
    "    for p in entities + events:\n",
    "        writer.writerow([p['~id'], p['~label'], p['label'], helpers.get_delim_string(p, 'labels'), p['type']])\n",
    "\n",
    "with open('graphdata/lpg/extraction_rels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~from\", \"~to\", \"label\", \"role\"])\n",
    "    for p in ee_rels + de_rels:\n",
    "        writer.writerow([p['~id'], p['~from'], p['~to'], p['~label'], p['role']])\n",
    "        \n",
    "# write docs for RDF\n",
    "rdf_file = helpers.rdf_open()\n",
    "    \n",
    "for d in distinct_roles:\n",
    "    helpers.rdf_write(rdf_file, URIRef(d), RDF.type, helpers.make_uri(\"ExtractionRole\"))\n",
    "    helpers.rdf_write(rdf_file, URIRef(d), RDFS.label, Literal(distinct_roles[d]))\n",
    "\n",
    "for p in entities + events:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDF.type, helpers.make_uri(\"Extraction\"))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDF.type, helpers.make_uri(p['~label']))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDF.type, helpers.make_uri(p['type']))\n",
    "\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDFS.label, Literal(p['label']))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), helpers.make_uri(\"xtype\"), Literal(p['type']))\n",
    "    for l in p['labels']:\n",
    "        helpers.rdf_write(rdf_file, URIRef(p['~id']), SKOS.altLabel, Literal(l))\n",
    "\n",
    "for p in de_rels:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~from']),helpers.make_uri(p['~label']), URIRef(p['~to']))\n",
    "        \n",
    "for p in ee_rels:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~from']),helpers.make_uri(p['role']), URIRef(p['~to']))\n",
    "    \n",
    "        \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/extactions.ttl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf66e3d-6035-4b74-b440-9ad17ea37cb7",
   "metadata": {},
   "source": [
    "### Entity Resolution, LLM Style\n",
    "Now let's try to link entities mentioned in Comprehend output to orgs, persons, industries, locations, products, and services in the graph. \n",
    "\n",
    "We'll get creative! Let's ask the LLM to find well-known URIs and alternate names for the entities and events discovered above.\n",
    "\n",
    "- resolved_entities.csv - Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "- resolution_links.csv -  Resolution matches that can be linked from extracted node to structured node.\n",
    "\n",
    "RDF in graphdata/rdf are:\n",
    "- resolved_entities.ttl -Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "845d3c9e-9123-4b0e-bca1-35d821abb015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "# Track entities to resolve\n",
    "er_entities={}\n",
    "for e in entities:\n",
    "    if e['type'] in ['PERSON', 'PERSON_TITLE', 'LOCATION', 'ORGANIZATION', 'STOCK_CODE']:\n",
    "        er_entities[e['~id']] = {'~id': e['~id'], 'label': e['label'], 'type': e['type']}\n",
    "\n",
    "count=len(er_entities)\n",
    "progress=0\n",
    "for e in er_entities:\n",
    "    if progress % 10 == 0:\n",
    "        print(str(progress))\n",
    "    er_entities[e]['resolution']=helpers.resolve_entities(er_entities[e]['label'])\n",
    "    progress += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd2079-d992-48c5-be15-930b1352ea65",
   "metadata": {},
   "source": [
    "### Find entities to match. Ask LLM to resolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9c64f-9d86-47bf-8f5e-4d05b21ffed8",
   "metadata": {},
   "source": [
    "### Save the result\n",
    "It takes time to produce. Save it to file in case we need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcbe80e4-8dc8-4562-b8a7-c18d2f9737b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('saved_er.json', 'w') as saved_er: \n",
    "    saved_er.write(json.dumps(er_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12fd22-f35f-4151-9541-945151793c2f",
   "metadata": {},
   "source": [
    "## Save these entities and see if we can resolve them once they land in the graph\n",
    "\n",
    "- resolved_entities.csv - Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "\n",
    "- extractions.ttl - Entities and events extracted from press releases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7d3dc69-50b8-4048-b64b-ced4db5b38cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_uri(uri):\n",
    "    return uri.startswith(\"http://\") or uri.startswith(\"https://\")\n",
    "\n",
    "resies=[]\n",
    "\n",
    "for eid in er_entities:\n",
    "    for index, r in enumerate(er_entities[eid]['resolution']):\n",
    "        resid=helpers.make_uri(f\"{eid}_res{index}\")\n",
    "        resies.append({\n",
    "            '~id': resid, \n",
    "            '~label': \"ExtractedEntityAltTerm\", \n",
    "            'label': r, \n",
    "            'is_uri': is_uri(r),\n",
    "            'entity_id': eid,\n",
    "            'edge_id': helpers.make_uri(f\"{eid}_res{index}_link\"),\n",
    "            'edge_type': \"LinkedAtlTerm\"\n",
    "        })\n",
    "        \n",
    "# write docs to CSV for LPG\n",
    "with open('graphdata/lpg/resolved_entities.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"label\"])\n",
    "    for e in resies:\n",
    "        writer.writerow([e['~id'], e['~label'], e['label']])\n",
    "\n",
    "with open('graphdata/lpg/extraction_links.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~from\", \"~to\", \"label\"])\n",
    "    for e in resies:\n",
    "        if e['is_uri']:\n",
    "            writer.writerow([e['edge_id'], e['entity_id'], e['label'], e['edge_type']])\n",
    "        \n",
    "# write docs for RDF\n",
    "rdf_file = helpers.rdf_open()\n",
    "\n",
    "for e in resies:\n",
    "    helpers.rdf_write(rdf_file, URIRef(e['~id']), RDF.type, helpers.make_uri(e['~label']))\n",
    "    helpers.rdf_write(rdf_file, URIRef(e['~id']), RDFS.label, Literal(e['label']))\n",
    "    if e['is_uri']:\n",
    "        helpers.rdf_write(rdf_file, URIRef(e['entity_id']), helpers.make_uri(e['edge_type']), URIRef(e['~label']))\n",
    "        \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/resolved_entities.ttl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b32a7-866b-45a8-9585-33cd602df5d8",
   "metadata": {},
   "source": [
    "## Make entities searchable\n",
    "That means embeddings of entities, events, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e4cfa-ec0b-48ba-adec-549bf5675ee3",
   "metadata": {},
   "source": [
    "## Create document summaries\n",
    "Ask LLM to summarize each press release document. \n",
    "\n",
    "Save summaries as .txt files in summaries folder.\n",
    "\n",
    "Also create CSV file keeping embeddings of each summary:\n",
    "\n",
    "- summaries.csv - References the documents from documents.csv but adding their embedding from summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a2e5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv, json\n",
    "progress=0\n",
    "with open(\"graphdata/lpg/summaries.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"embedding:vector\"])\n",
    "\n",
    "    jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "    for index, row in jsonObj.iterrows():\n",
    "    \n",
    "        if progress % 5 == 0:\n",
    "            print(str(progress))\n",
    "\n",
    "        # extract metadata about current press release\n",
    "        metadata=row['metadata']\n",
    "        m_keywords=metadata['keywords']\n",
    "        m_title=metadata['title']\n",
    "        m_doc=metadata['document_id']\n",
    "\n",
    "        summary=helpers.summarize(row['raw_text'])\n",
    "        summary_embedding=helpers.embedding_string(helpers.make_embedding(summary))\n",
    "    \n",
    "        with open(f\"summaries/{m_doc}.txt\", \"w\") as f:\n",
    "            f.write(summary)\n",
    "\n",
    "        writer.writerow([make_document_uri(m_doc), \"Document\", summary_embedding])\n",
    "        \n",
    "        progress+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9679b-18af-4845-ab2d-ad430304efbc",
   "metadata": {},
   "source": [
    "## Make chunks of docs\n",
    "Split press release documents into chunks, create embeddings for each chunk, and link the chunk to the document in the graph.\n",
    "\n",
    "Writes files to chunks folder.\n",
    "\n",
    "Creates the following CSV files:\n",
    "\n",
    "- chunks.csv - Document chunks and their embeddings.\n",
    "- chunk2doc.csv -Link chunks to docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a00564f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_chunk_to_file(docfile, chunkidx, content):\n",
    "    file_name=f\"chunks/{docfile}_{str(chunkidx)}\"\n",
    "    text_file = open(file_name, \"w\")\n",
    "    text_file.write(content)\n",
    "    text_file.close()\n",
    "    return file_name\n",
    "\n",
    "all_splits=helpers.make_doc_splits(\"documents\")\n",
    "chunk2doc={}\n",
    "with open(\"graphdata/lpg/chunks.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"doc\", \"chunkidx:Integer\", \"embedding:vector\"])\n",
    "    \n",
    "    for index, s in enumerate(all_splits):\n",
    "        content=s.page_content\n",
    "        doc=s.metadata['source'].split(\"/\")[-1]\n",
    "        file_name = write_chunk_to_file(doc, index, content)\n",
    "        embedding=helpers.embedding_string(helpers.make_embedding(content))\n",
    "        chunk_local =f\"{doc}_{index}\" \n",
    "        chunk_uri = helpers.make_uri(f\"Chunk/{chunk_local}\")\n",
    "        doc_uri = make_document_uri(doc)\n",
    "        writer.writerow([chunk_uri, \"Chunk\", doc, index, embedding])\n",
    "        chunk2doc[chunk_uri] = {\n",
    "            'chunk_uri': chunk_uri,\n",
    "            'doc_uri': doc_uri,\n",
    "            'chunk_local': chunk_local,\n",
    "            'doc_local': doc\n",
    "        }\n",
    "        \n",
    "# links chunks to docs\n",
    "with open(\"graphdata/lpg/chunk2doc.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\",\"~from\", \"~to\", \"~label\"])\n",
    "    \n",
    "    for cd in chunk2doc:\n",
    "        cdrec = chunk2doc[cd]\n",
    "        edge_id=helpers.make_uri(f\"chunkToDoc_{cdrec['chunk_local']}_{cdrec['doc_local']}\")\n",
    "        writer.writerow([edge_id, cdrec['chunk_uri'], cdrec['doc_uri'], \"belongsToDocument\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb4a60-0d11-4a74-ae1a-1cd7af8b0b62",
   "metadata": {},
   "source": [
    "## Make entity embeddings so I can search them better\n",
    "\n",
    "In scope:\n",
    "- taxonomical concept\n",
    "- orgs, persons, industries, services, products, locations\n",
    "- any loose structured objects \n",
    "- extracted entities\n",
    "- extracted events\n",
    "\n",
    "Not in scope:\n",
    "- relationships among structured entities\n",
    "- roles in extraction\n",
    "\n",
    "When a user queries, we extract entities from the QUESTION. We want reasonable searchability of those entities. The not-in-scope data is a small enumerated set. We can prompt the LLM to express predicates in the question from that set.\n",
    "\n",
    "In the code below, we populate the following:\n",
    "\n",
    "- entity_embeddings.csv - Embeddings of entities for searchability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d016fc43-bf87-4eec-8676-3749670eaf62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orgs\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'ulabels': 4, 'types': 5, 'typeLabels': 6, 'seeAlsos': 7, 'seeAlsoTypes': 8, 'seeAlsoTypeLabels': 9, 'seeAlsoLabels': 10, 'leType': 11, 'leStatus': 12}\n",
      "AbeBooks AbeBooks AbeBooks Subsidiary  \n",
      "Amazon (company) Amazon (company) Amazon.com, Inc. amazon.com Amazon_(company) Public company Criticism of Amazon List of Amazon locations \n",
      "BookFinder.com BookFinder.com BookFinder.com Comparison shopping website  \n",
      "  IberLibro   \n",
      "LibraryThing LibraryThing LibraryThing   \n",
      "A9.com A9.com A9.com, Inc. A9.com Search engine technology  \n",
      "Alexa Internet Alexa Internet Alexa Internet, Inc. Alexa_Internet   \n",
      "Amazon.com Amazon.com Amazon.com   \n",
      "Amazon Air Amazon Air Amazon_Air   \n",
      "Amazon Books Amazon Books Amazon_Books Subsidiary  \n",
      "Amazon Fresh Amazon Fresh Amazon_Fresh Subsidiary  \n",
      "Amazon Games Amazon Games Amazon_Games   \n",
      "Amazon Lab126 Amazon Lab126 Amazon Lab126, Inc. Amazon_Lab126 Subsidiary  \n",
      "Amazon Logistics Amazon Logistics Amazon_Logistics   \n",
      "Amazon Pharmacy Amazon Pharmacy Amazon_Pharmacy Subsidiary  \n",
      "Amazon Publishing Amazon Publishing Amazon_Publishing   \n",
      "Amazon Robotics Amazon Robotics Amazon_Robotics   \n",
      "Amazon Studios Amazon Studios Amazon_Studios Subsidiary List of Amazon Prime Video original programming \n",
      "Amazon Web Services Amazon Web Services Amazon Web Services, Inc. Amazon_Web_Services Subsidiary  \n",
      "Audible (service) Audible (service) Audible, Inc. Audible_(service)   \n",
      "Blink Home Blink Home Immedia Semiconductor, LLC Blink_Home Subsidiary  \n",
      "Body Labs Body Labs Body_Labs Subsidiary  \n",
      "Book Depository Book Depository The Book Depository Ltd. Book_Depository   \n",
      "ComiXology ComiXology ComiXology Subsidiary  \n",
      "Digital Photography Review Digital Photography Review Digital_Photography_Review   \n",
      "Goodreads Goodreads Goodreads Book  \n",
      "Graphiq Graphiq Graphiq   \n",
      "IMDb IMDb IMDb   \n",
      "MGM Holdings MGM Holdings MGM Holdings, Inc. MGM_Holdings Subsidiary  \n",
      "PillPack PillPack PillPack, Inc. PillPack Division (business)  \n",
      "Ring Inc. Ring Inc. Ring_Inc.   \n",
      "Souq.com Souq.com Souq.com   \n",
      "Twitch Interactive Twitch Interactive Twitch_Interactive   \n",
      "Whole Foods Market Whole Foods Market Whole Foods Market IP, Inc. Whole_Foods_Market Subsidiary  \n",
      "Woot Woot Woot, Inc. Woot Subsidiary  \n",
      "Zappos Zappos Zappos.com Zappos Subsidiary  \n",
      "Zoox Inc Zoox Inc Zoox_Inc   \n",
      "Epic Games, Inc. Epic Games, Inc. Epic Games Epic_Games Privately held company Fortnite Battle Royale \n",
      "Lockheed Martin Lockheed Martin Lockheed Martin Corporation Lockheed_Martin Public company  \n",
      "Lockheed Martin Canada Lockheed Martin Canada Lockheed_Martin_Canada   \n",
      "Lockheed Martin UK Lockheed Martin UK Lockheed_Martin_UK   \n",
      "Sikorsky Aircraft Sikorsky Aircraft Sikorsky_Aircraft Subsidiary  \n",
      "Ring LLC Ring LLC Ring (company) Ring_(company) Subsidiary  \n",
      "Rite Aid Corporation Rite Aid Corporation Rite Aid Rite_Aid Public company  \n",
      "The Bartell Drug Company The Bartell Drug Company Bartell Drugs Bartell_Drugs Subsidiary  \n",
      "Rivian Automotive, Inc. Rivian Automotive, Inc. Rivian Rivian Public company  \n",
      "Saint Mary's Hospital, Manchester Saint Mary's Hospital, Manchester Saint Mary's Hospital Saint_Mary's_Hospital,_Manchester   \n",
      "Salesforce, Inc. Salesforce, Inc. Salesforce Salesforce Public company  \n",
      "Shutterfly Shutterfly Shutterfly, LLC. Shutterfly   \n",
      "Spire Global, Inc. Spire Global, Inc. Spire Global Spire_Global Public company  \n",
      "Standard Bank Group Limited Standard Bank Group Limited Standard Bank Standard_Bank Public company  \n",
      "The Globe and Mail The Globe and Mail The_Globe_and_Mail   \n",
      "Tsawwassen First Nation Tsawwassen First Nation Tsawwassen_First_Nation   \n",
      "Verizon Communications Verizon Communications Verizon Communications Inc Verizon Communications Inc. Verizon_Communications  Verizon FiOS \n",
      "Visy Visy Visy Industries Visy Privately held company  \n",
      "persons\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'ulabels': 4, 'types': 5, 'typeLabels': 6, 'seeAlsos': 7, 'seeAlsoTypes': 8, 'seeAlsoTypeLabels': 9, 'seeAlsoLabels': 10}\n",
      "Chief executive officer Chief executive officer Chief_executive_officer   \n",
      "Andy Jassy Andy Jassy Andy_Jassy   \n",
      "Jeff Bezos Jeff Bezos Jeff_Bezos  The Washington Post Daily newspaper\n",
      "President (corporate title) President (corporate title) President President_(corporate_title)  -elect \n",
      "Chairman Chairman Chairman   \n",
      "Chief Creative Officer Chief Creative Officer Chief_Creative_Officer   \n",
      "Chief technical officer Chief technical officer Chief_technical_officer   \n",
      "Mark Rein (software executive) Mark Rein (software executive) Mark Rein Mark_Rein_(software_executive)   \n",
      "James D. Taiclet James D. Taiclet James_D._Taiclet   \n",
      "CEO CEO CEO   \n",
      "products\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'ulabels': 4, 'types': 5, 'typeLabels': 6, 'seeAlsos': 7, 'seeAlsoTypes': 8, 'seeAlsoTypeLabels': 9, 'seeAlsoLabels': 10}\n",
      "Book Book Book  Publishing Stylus Palm-leaf manuscript \n",
      "Collectable Collectable Collectable  NFTs \n",
      "Ephemera Ephemera Ephemera   \n",
      "Fine art Fine art Fine_art  List of art schools \n",
      "Out of print books Out of print books Out_of_print_books   \n",
      "Rare book Rare book Rare_book   \n",
      "Textbooks Textbooks Textbooks   \n",
      "Used book Used book Used_book   \n",
      "Amazon Echo Amazon Echo Amazon_Echo Smart speaker Amazon Alexa \n",
      "Amazon Fire TV Amazon Fire TV Amazon_Fire_TV Microconsole Digital media player  \n",
      "Amazon Fire tablet Amazon Fire tablet Amazon_Fire_tablet   \n",
      "Amazon Kindle Amazon Kindle Amazon_Kindle E-reader  \n",
      "Fire OS Fire OS Fire_OS  Fire HD Amazon Fire HD Tablet computer\n",
      "Bink Video Bink Video Bink_Video   \n",
      "Epic Games Store Epic Games Store Epic_Games_Store   \n",
      "Fortnite Fortnite Fortnite  Fortnite: Save the World Fortnite Battle Royale \n",
      "Gears of War Gears of War Gears_of_War  List of Gears of War media \n",
      "Unreal (video game series) Unreal (video game series) Unreal_(video_game_series)   \n",
      "Unreal Engine Unreal Engine UnrealScript Unreal_Engine  Epic Games v. Apple \n",
      "Atlas V Atlas V Atlas_V   \n",
      "Lockheed Martin C-130J Super Hercules Lockheed Martin C-130J Super Hercules Lockheed_Martin_C-130J_Super_Hercules Military transport aircraft Aerial refuelling TACAMO \n",
      "Lockheed Martin F-35 Lightning II Lockheed Martin F-35 Lightning II Lockheed_Martin_F-35_Lightning_II Multirole combat aircraft Joint Combat Aircraft \n",
      "Pharmacy Pharmacy Pharmacy  List of pharmacy associations Community pharmacy \n",
      "Car battery Car battery Car_battery   \n",
      "Electric car Electric car Electric_car  The United States Plug-in electric vehicles France Vehicle classification by propulsion system Phase-out of fossil fuel vehicles French Republic \n",
      "Bureau de change Bureau de change Bureau_de_change   \n",
      "Commercial Banking Commercial Banking Commercial_Banking   \n",
      "Insurance Insurance Insurance   \n",
      "Investment Banking Investment Banking Investment_Banking   \n",
      "Investment Management Investment Management Investment_Management   \n",
      "Private Banking Private Banking Private_Banking   \n",
      "Retail Banking Retail Banking Retail_Banking   \n",
      "Wealth Management Wealth Management Wealth_Management   \n",
      "industries\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'ulabels': 4, 'types': 5, 'typeLabels': 6, 'seeAlsos': 7, 'seeAlsoTypes': 8, 'seeAlsoTypeLabels': 9, 'seeAlsoLabels': 10}\n",
      "Internet Internet Internet  List of countries by number of Internet users Internet censorship Signals intelligence Social networking service Culture of fear Global Internet usage \n",
      "E-commerce E-commerce E-commerce   \n",
      "Artificial intelligence Artificial intelligence Artificial_intelligence  Embodied cognition \n",
      "Cloud Computing Cloud Computing Cloud_Computing   \n",
      "Consumer electronics Consumer electronics Consumer_electronics  Electronic packaging Electronics industry Electronics technician List of electronics brands Consumer electronics store Japan Software development \n",
      "Digital distribution Digital distribution Digital_distribution   \n",
      "Entertainment Entertainment Entertainment   \n",
      "Self-driving cars Self-driving cars Self-driving_cars   \n",
      "Supermarket Supermarket Supermarket  Grocery store \n",
      "Grocery store Grocery store Grocery_store  List of hypermarkets List of online grocers \n",
      "Health food store Health food store Health_food_store   \n",
      "Video game industry Video game industry Video_game_industry  List of video game awards Regional lockout Eighth generation of video game consoles Ninth generation of video game consoles Video game journalism First generation of video game consoles Fourth generation of video game consoles Golden age of video arcade games Sixth generation of video game consoles \n",
      "Retail Retail Retail  List of largest retail companies Strategic planning \n",
      "Automotive industry Automotive industry Automotive_industry  List of car brands List of countries by motor vehicle production 2009–11 Toyota vehicle recalls List of manufacturers by motor vehicle production Automotive industry by country \n",
      "Energy storage Energy storage Energy_storage   \n",
      "Cloud computing Cloud computing Cloud_computing  Hybrid cloud storage \n",
      "Consulting Consulting Consulting   \n",
      "Enterprise software Enterprise software Enterprise system Enterprise_software   \n",
      "Aerospace Aerospace Aerospace   \n",
      "Banking Banking Banking   \n",
      "Packaging Packaging Packaging   \n",
      "locations\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'ulabels': 4, 'types': 5, 'typeLabels': 6, 'seeAlsos': 7, 'seeAlsoTypes': 8, 'seeAlsoTypeLabels': 9, 'seeAlsoLabels': 10}\n",
      "Victoria Victoria The Corporation of the City of Victoria Victoria, British Columbia Victoria,_British_Columbia List of cities in British Columbia  \n",
      "Bethesda, Maryland Bethesda, Maryland Bethesda,_Maryland Census-designated place  \n",
      "Santa Monica, California Santa Monica, California City of Santa Monica Santa_Monica,_California List of municipalities in California Los Angeles Sports List of municipalities in California\n",
      "Philadelphia, Pennsylvania Philadelphia, Pennsylvania Philadelphia,_Pennsylvania   \n",
      "United States United States United States of America United_States  American nationalism The United States Lost Cause of the Confederacy Health care Newspapers List of U.S. states by population \n",
      "Salesforce Tower Salesforce Tower Salesforce_Tower   \n",
      "San Francisco, California San Francisco, California San_Francisco,_California   \n",
      "City of Johannesburg City of Johannesburg Johannesburg Johannesburg City Johannesburg City of Johannesburg City\n",
      "Standard Bank Centre Standard Bank Centre Standard_Bank_Centre   \n",
      "South Africa South Africa Republic of South Africa South_Africa  Union of South Africa South African wine Wildlife of South Africa Portuguese discoveries Official names of South Africa \n",
      "services\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'ulabels': 4, 'types': 5, 'typeLabels': 6, 'seeAlsos': 7, 'seeAlsoTypes': 8, 'seeAlsoTypeLabels': 9, 'seeAlsoLabels': 10}\n",
      "Online shopping Online shopping Online_shopping   \n",
      "Amazon.ca Amazon.ca Amazon.ca   \n",
      "Amazon Alexa Amazon Alexa Amazon_Alexa  Internet privacy \n",
      "Amazon Appstore Amazon Appstore Amazon_Appstore  Apple Inc. litigation \n",
      "Amazon Prime Amazon Prime Amazon_Prime Subscription business model  \n",
      "Amazon Prime Video Amazon Prime Video Amazon_Prime_Video Over-the-top media service  \n",
      "Amazon Web Services Amazon Web Services Amazon Web Services, Inc. Amazon_Web_Services Subsidiary  \n",
      "Ring LLC Ring LLC Ring (company) Ring_(company) Subsidiary  \n",
      "Twitch Interactive, Inc. Twitch Interactive, Inc. Twitch (service) Twitch_(service) Video on demand Live streaming Twitch Rivals Parasocial interaction \n",
      "Amazon Luna Amazon Luna Amazon_Luna   \n",
      "Amazon Music Amazon Music Amazon_Music   \n",
      "Amazon Pay Amazon Pay Amazon_Pay Subsidiary  \n",
      "Electric vehicle charging network Electric vehicle charging network Electric_vehicle_charging_network   \n",
      "Vehicle insurance Vehicle insurance Vehicle_insurance   \n",
      "Cloud computing Cloud computing Cloud_computing  Hybrid cloud storage \n",
      "taxonomy_concepts\n",
      "{'~id': 0, '~label': 1, 'prefLabel': 2, 'altLabels': 3, 'broaders': 4}\n",
      "Internet Web Online technology\n",
      "E-Commerce Electronic Commerce Online Shopping internet\n",
      "Artificial Intelligence AI technology\n",
      "Cloud Computing Cloud technology\n",
      "Consumer Electronics Home Electronics Personal Electronics technology\n",
      "Digital Distribution Electronic Distribution technology\n",
      "Entertainment Amusement Recreation industry\n",
      "Self-Driving Cars Autonomous Vehicles Driverless Cars technology automotive industry\n",
      "Supermarket Grocery Superstore retail\n",
      "Grocery Store Food Market retail\n",
      "Health Food Store Natural Food Store Organic Grocery Store retail\n",
      "Video Game Industry Gaming Industry entertainment technology\n",
      "Retail Retailing industry\n",
      "Automotive Industry Car Industry Motor Vehicle Industry industry\n",
      "Energy Storage Power Storage technology\n",
      "Cloud Computing Cloud technology\n",
      "Consulting Advisory Services industry\n",
      "Enterprise Software Business Software technology\n",
      "Aerospace Aeronautics and Space industry\n",
      "Banking Financial Services industry\n",
      "Packaging Packing industry\n",
      "extractions\n",
      "{'~id': 0, '~label': 1, 'label': 2, 'labels': 3, 'type': 4}\n",
      "Whole Foods Market Whole Foods Market Whole Foods Market, Inc.\n",
      "Amazon Amazon\n",
      "Jun. 16, 2017 Jun. 16, 2017 today\n",
      "NASDAQ:AMZN NASDAQ:AMZN\n",
      "NASDAQ:WFM NASDAQ:WFM\n",
      "they they\n",
      "John Mackey John Mackey\n",
      "CEO CEO\n",
      "parties parties\n",
      "during the second half of 2017 during the second half of 2017\n",
      "Amazon Amazon\n",
      "Whole Foods Market Whole Foods Market\n",
      "personnel personnel\n",
      "Investors Investors\n",
      "participants participants individuals\n",
      "Whole Foods Market Whole Foods Market\n",
      "Amazon Amazon Amazon.com\n",
      "on Monday August 28, 2017 on Monday August 28, 2017\n",
      "over time over time\n",
      "companies companies\n",
      "members members\n",
      "John Mackey John Mackey\n",
      "CEO CEO\n",
      "Whole Foods Market Whole Foods Market supermarket\n",
      "SEATTLE SEATTLE Minneapolis\n",
      "Jun. 9, 2016 Jun. 9, 2016 today\n",
      "Amazon Amazon\n",
      "Code Savvy Code Savvy I Raymond Dehn\n",
      "employees employees\n",
      "software engineers software engineers\n",
      "workforce workforce\n",
      "Amazon Amazon\n",
      "businesses businesses they\n",
      "developers developers\n",
      "startups startups\n",
      "entrepreneurs entrepreneurs\n",
      "Sharks Sharks\n",
      "startups startups\n",
      "startups startups\n",
      "Sharks Sharks tycoons\n",
      "businesses businesses\n",
      "America America\n",
      "Amazon Amazon\n",
      "millions of dollars millions of dollars\n",
      "Prime Prime Amazon\n",
      "this year this year\n",
      "members members\n",
      "Amazon Amazon\n",
      "Amazon Amazon\n",
      "currently currently\n",
      "this year this year\n",
      "applicants applicants\n",
      "applicants applicants\n",
      "Amazon Amazon Alexa we\n",
      "Amazon Amazon it\n",
      "today today\n",
      "eero eero\n",
      "NASDAQ:AMZN NASDAQ:AMZN\n",
      "they they\n",
      "Amazon Amazon\n",
      "Amazon Amazon\n",
      "Amazon Amazon we\n",
      "U.S. U.S. country\n",
      "in the coming weeks in the coming weeks\n",
      "Candidates Candidates\n",
      "Since 2010 Since 2010\n",
      "businesses businesses\n",
      "Plymouth Housing Plymouth Housing\n",
      "Seattle Seattle\n",
      "Arlington Community Foundation Arlington Community Foundation\n",
      "today today Today\n",
      "Amazon Amazon\n",
      "nonprofits nonprofits\n",
      "community community\n",
      "neighbors neighbors\n",
      "we we Amazon\n",
      "Plymouth Housing Plymouth Housing\n",
      "employees employees\n",
      "nonprofits nonprofits\n",
      "Washington State Washington State\n",
      "employees employees\n",
      "nonprofits nonprofits\n",
      "regions regions\n",
      "Jul. 22, 2019 Jul. 22, 2019 today\n",
      "Amazon.com, Inc. Amazon.com, Inc. Amazon company\n",
      "Ohio Ohio\n",
      "Amazonians Amazonians\n",
      "roles roles\n",
      "Ohio Ohio\n",
      "JobsOhio JobsOhio\n",
      "Ohioans Ohioans\n",
      "Rossford Rossford\n",
      "members members\n",
      "Since 2010 Since 2010\n",
      "facility facility center\n",
      "Allegheny County Allegheny County\n",
      "Jul. 30, 2019 Jul. 30, 2019 today\n",
      "Amazon.com, Inc. Amazon.com, Inc. Amazon\n",
      "Pennsylvanian Pennsylvanian Pennsylvania\n",
      "Since 2010 Since 2010\n",
      "state state\n",
      "Since 2010 Since 2010\n",
      "Amazon Amazon we\n",
      "companies companies\n",
      "signatories signatories\n",
      "companies companies\n",
      "Rivian Rivian\n",
      "today today\n",
      "today today\n",
      "Amazon Amazon business\n",
      "companies companies\n",
      "Zac Brown Zac Brown Zac he\n",
      "DemerBox DemerBox\n",
      "Amazon Amazon\n",
      "Alliance for Education Alliance for Education Alliance\n",
      "city city Seattle\n",
      "funds funds\n",
      "Amazon Amazon\n",
      "Colorado Colorado state\n",
      "since 2016 since 2016\n",
      "Apr. 30, 2019 Apr. 30, 2019 today\n",
      "Denver Denver\n",
      "workforce workforce\n",
      "Colorado Colorado state\n",
      "over the last three years over the last three years\n",
      "Whole Foods Market Whole Foods Market\n",
      "Amazon Amazon\n",
      "Over the past seven years Over the past seven years\n",
      "U.S. U.S.\n",
      "expert expert\n",
      "Amazon Amazon Amazon Hub We\n",
      "Counter Counter\n",
      "Rite Aid Rite Aid\n",
      "organizations organizations\n",
      "We We\n",
      "Amazon Literary Partnership Amazon Literary Partnership\n",
      "this year this year\n",
      "Amazon Amazon\n",
      "Lambda Literary Lambda Literary Lambda\n",
      "current current\n",
      "Amazon Amazon company us\n",
      "workforce workforce\n",
      "communities communities\n",
      "employees employees\n",
      "employees employees\n",
      "Amazon Amazon\n",
      "SMB SMB\n",
      "Developers Developers\n",
      "in 2017 in 2017\n",
      "Since 2011 Since 2011\n",
      "tens of billions tens of billions\n",
      "companies companies\n",
      "Amazon Amazon\n",
      "employees employees\n",
      "agencies agencies\n",
      "seasonal seasonal\n",
      "Amazon Amazon\n",
      "businesses businesses\n",
      "This year This year\n",
      "owners owners\n",
      "communities communities\n",
      "Yedi Houseware Yedi Houseware\n",
      "12 years ago 12 years ago\n",
      "this year this year\n",
      "businesses businesses\n",
      "State Street Corporation State Street Corporation We\n",
      "institutional investors institutional investors\n",
      "Amazon.com, Inc. Amazon.com, Inc. it\n",
      "amount amount\n",
      "we we\n",
      "customers customers\n",
      "AWS AWS Amazon Web Services\n",
      "Emirates NBD Emirates NBD\n",
      "Amazon Amazon\n",
      "employee employee\n",
      "employee employee they\n",
      "businesses businesses\n",
      "drivers drivers\n",
      "Since 2011 Since 2011\n",
      "tens of billions tens of billions\n",
      "small and medium-sized businesses small and medium-sized businesses SMBs\n",
      "The Globe and Mail The Globe and Mail\n",
      "Thomson family Thomson family\n",
      "Amazon Amazon\n",
      "Amazon Amazon We\n",
      "small businesses small businesses they\n",
      "talent talent\n",
      "business business\n",
      "employees employees\n",
      "Sylvia Day Sylvia Day\n",
      "Amazon Publishing Amazon Publishing\n",
      "Apr. 29, 2019 Apr. 29, 2019 today\n",
      "Romance Romance Montlake Romance\n",
      "Anh Schluep Anh Schluep\n",
      "editor editor\n",
      "Standard Bank Group Standard Bank Group bank\n",
      "SBK SBK\n",
      "SNB SNB\n",
      "in 2019 in 2019\n",
      "September 27, 2018 September 27, 2018 today\n",
      "Amazon Amazon\n",
      "currently currently\n",
      "associates associates Associates\n",
      "Columbians Columbians\n",
      "people people\n",
      "Tsawwassen First Nation Tsawwassen First Nation\n",
      "infrastructure infrastructure\n",
      "Lockheed Martin Lockheed Martin\n",
      "AWS AWS\n",
      "companies companies we\n",
      "Spire Global Spire Global Spire Global, Inc.\n",
      "Lockheed Martin Lockheed Martin\n",
      "AWS AWS biotechnology company Amazon Web Services, Inc. Amgen\n",
      "technologists technologists\n",
      "Amazon Amazon Amazon Handmade\n",
      "employees employees\n",
      "AWS AWS Amazon Web Services, Inc.\n",
      "Jul. 17, 2018 Jul. 17, 2018 Today\n",
      "Epic Games Epic Games developer We\n",
      "world world globe\n",
      "AWS AWS Guardian it Amazon Web Services, Inc.\n",
      "businesses businesses\n",
      "nearly two decades ago nearly two decades ago\n",
      "Amazon Amazon\n",
      "businesses businesses\n",
      "in October 2016 in October 2016\n",
      "members members\n",
      "Amazon Web Services, Inc. Amazon Web Services, Inc. AWS\n",
      "Salesforce Salesforce\n",
      "companies' companies'\n",
      "Openbank Openbank\n",
      "Compass Group Compass Group\n",
      "associates associates\n",
      "Visy Visy company We subsidiary AWS\n",
      "Australian Australian Australia\n",
      "Shutterfly, Inc. Shutterfly, Inc. Shutterfly\n",
      "Lifetouch Lifetouch\n",
      "in the second quarter of 2018 in the second quarter of 2018\n",
      "AWS AWS Amazon Web Services, Inc. Verizon Communications Verizon\n",
      "Verizon Verizon\n",
      "Amazon Amazon\n",
      "in 2019 in 2019\n",
      "$15 billion $15 billion\n",
      "Nov. 13, 2017 Nov. 13, 2017 today\n",
      "it it Amazon Studios\n",
      "Amazon Amazon\n",
      "organizations organizations\n",
      "country country\n",
      "AWS AWS Amazon Web Services\n",
      "Ireland Ireland\n",
      "employer employer Amazon\n",
      "tens of millions of dollars tens of millions of dollars\n",
      "communities communities\n",
      "students students\n",
      "We We we\n",
      "Amazon Amazon\n",
      "organizations organizations\n",
      "country country\n",
      "Amazon Amazon We\n",
      "Texas Texas state\n",
      "since 2011 since 2011\n",
      "employees employees\n",
      "Mar. 28, 2019 Mar. 28, 2019 today\n",
      "In the last four years In the last four years\n",
      "Austin Austin\n",
      "Since 2011 Since 2011\n",
      "Texas Texas state\n",
      "2011-2017 2011-2017\n",
      "Over the past seven years Over the past seven years\n",
      "U.S. U.S.\n",
      "Amazon Amazon Engineer Amazon Future Engineer\n",
      "Amazon Amazon\n",
      "businesses businesses\n",
      "employees employees\n",
      "Amazon Amazon\n",
      "organizations organizations\n",
      "country country\n",
      "Company Company Amazon\n",
      "by 2028 by 2028\n",
      "U.S. U.S. United States\n",
      "Oct. 16, 2018 Oct. 16, 2018 today\n",
      "Closed Loop Fund Closed Loop Fund\n",
      "by 2020 by 2020\n",
      "Amazon Amazon\n",
      "Transparency Transparency\n",
      "Cowin Cowin we\n",
      "Amazon Amazon\n",
      "businesses businesses\n",
      "in 2011 in 2011\n",
      "businesses businesses\n",
      "capital capital\n",
      "business business\n",
      "small businesses small businesses\n",
      "businesses businesses\n",
      "businesses businesses\n",
      "Japan Japan\n",
      "business business\n",
      "LonoLife LonoLife\n",
      "Businesses Businesses\n",
      "Amazon Amazon company Amazon.com\n",
      "Seattle Seattle city\n",
      "employees employees\n",
      "Amazon HQ2 Amazon HQ2\n",
      "tens of billions of dollars tens of billions of dollars\n",
      "SEATTLE SEATTLE\n",
      "Sep. 7, 2017 Sep. 7, 2017 today\n",
      "North America North America\n",
      "tens of billions of dollars tens of billions of dollars\n",
      "community community\n",
      "from 2010 through 2016 from 2010 through 2016\n",
      "dollar dollar\n",
      "executives executives\n",
      "employees employees they\n",
      "Amazon Future Engineer Amazon Future Engineer\n",
      "Amazon Amazon\n",
      "organizations organizations\n",
      "country country\n",
      "Amazon Studios Amazon Studios\n",
      "Amazon Amazon\n",
      "billions of dollars billions of dollars\n",
      "businesses businesses\n",
      "Amazon Handmade Amazon Handmade\n",
      "in 2015 in 2015\n",
      "Amazon Amazon\n",
      "Mar. 25, 2019 Mar. 25, 2019 today\n",
      "NASDAQ: AMZN NASDAQ: AMZN\n",
      "businesses businesses\n",
      "people people\n",
      "Amazon Amazon company\n",
      "millions of dollars millions of dollars\n",
      "jobs jobs\n",
      "Thomas & Mercer Thomas & Mercer\n",
      "Amazon Publishing Amazon Publishing it\n",
      "Grace Doyle Grace Doyle\n",
      "North American North American\n",
      "Julia Sommerfeld Julia Sommerfeld\n",
      "World English World English\n",
      "Amazon Original Stories Amazon Original Stories\n",
      "Koontz Koontz author\n",
      "Richard Pine Richard Pine\n",
      "Megan Bookman Megan Bookman Megan\n",
      "Woody Woody boy\n",
      "killers killers\n",
      "family family\n",
      "in spring 2020 in spring 2020\n",
      "February 4, 2008 February 4, 2008 today\n",
      "Amazon.com Amazon.com Amazon\n",
      "LOVEFiLM International LOVEFiLM International LOVEFiLM\n",
      "Europe Europe\n",
      "United Kingdom United Kingdom UK\n",
      "Germany Germany German\n",
      "staff staff\n",
      "Amazon Studios Amazon Studios Amazon Publishing\n",
      "earlier this year earlier this year\n",
      "Amazon Original Stories Amazon Original Stories\n",
      "Amazon Amazon\n",
      "NASDAQ:AMZN NASDAQ:AMZN\n",
      "PillPack PillPack\n",
      "today today\n",
      "parties parties\n",
      "during the second half of 2018 during the second half of 2018\n",
      "company company Amazon\n",
      "Kentucky Kentucky\n",
      "people people\n",
      "Today Today today\n",
      "employees employees\n",
      "Air Air Prime Air\n",
      "Kentucky Kentucky state\n",
      "employees employees they\n",
      "companies companies\n",
      "employees employees\n",
      "CVG CVG\n",
      "Amazon Amazon\n",
      "businesses businesses\n",
      "billions of dollars billions of dollars\n",
      "Apr. 9, 2019 Apr. 9, 2019 today\n",
      "businesses businesses\n",
      "Amazon Studios Amazon Studios Amazon\n",
      "currently currently\n",
      "people people\n",
      "AmazonCrossing AmazonCrossing\n",
      "Apr. 17, 2018 Apr. 17, 2018 today\n",
      "Amazon Amazon\n",
      "New York City New York City\n",
      "Northern Virginia Northern Virginia\n",
      "$5 billion $5 billion\n",
      "Amazon Amazon Amazon Marketplace Appstore\n",
      "in 2019 in 2019 in 2018\n",
      "this year this year\n",
      "businesses businesses\n",
      "businesses businesses\n",
      "businesses businesses\n",
      "Amazon Amazon\n",
      "business business\n",
      "hundreds of millions of dollars hundreds of millions of dollars\n",
      "personnel personnel\n",
      "Project Zero Project Zero\n",
      "Amazon Amazon company\n",
      "in 2019 in 2019 this year\n",
      "Amazon Future Engineer Amazon Future Engineer Amazon AmazonFutureEngineer.com\n",
      "organizations organizations\n",
      "country country\n",
      "Oct. 10, 2019 Oct. 10, 2019 today\n",
      "Amazon.com, Inc. Amazon.com, Inc. Amazon City\n",
      "Idaho Idaho\n",
      "Nampa Nampa\n",
      "Amazonians Amazonians\n",
      "company company Amazon Robotics’ Amazon\n",
      "MA MA state\n",
      "since 2011 since 2011\n",
      "Nov. 6, 2019 Nov. 6, 2019 today\n",
      "companies companies\n",
      "workers workers\n",
      "Westborough Westborough\n",
      "Since 2011 Since 2011\n",
      "country country U.S.\n",
      "Amazon Amazon\n",
      "St. Mary’s Center for Women and Children St. Mary’s Center for Women and Children\n",
      "Dorchester Dorchester\n",
      "Since 2010 Since 2010\n",
      "Amazon Amazon\n",
      "Dec. 3, 2019 Dec. 3, 2019 today\n",
      "Illinois Illinois\n",
      "Commonwealth of Commonwealth of\n",
      "Amazon Amazon\n",
      "U.S. U.S.\n",
      "this year this year\n",
      "beyond beyond\n",
      "Commonwealth of Virginia Commonwealth of Virginia Virginia\n",
      "Amazon Amazon\n",
      "Commonwealth of Virginia Commonwealth of Virginia Virginia\n",
      "Amazon Amazon company\n",
      "Amazon Amazon\n",
      "associates associates\n",
      "drivers drivers\n",
      "customers customers\n",
      "charitable organization charitable organization\n",
      "Amazon Amazon Amazon.com, Inc.\n",
      "Amazon Amazon\n",
      "billions of dollars billions of dollars\n",
      "Dream Big Printables Dream Big Printables\n",
      "employees employees\n",
      "In 2019 In 2019\n",
      "businesses businesses\n",
      "businesses businesses\n",
      "Dec. 1, 2008 Dec. 1, 2008 today\n",
      "Amazon.com, Inc. Amazon.com, Inc. Amazon.com\n",
      "AbeBooks AbeBooks\n",
      "on Aug. 1, 2008 on Aug. 1, 2008\n",
      "Victoria Victoria\n",
      "Amazon.com Amazon.com\n",
      "business business\n",
      "Amazon.com Amazon.com\n",
      "football league football league Bundesliga\n",
      "Amazon Prime Amazon Prime Prime Video Prime\n",
      "Canadian Canadian\n",
      "students students\n",
      "Prime Student Prime Student\n",
      "Amazon Amazon\n",
      "today today\n",
      "2 billion 2 billion\n",
      "in 2016 in 2016 2016\n",
      "jobs jobs\n",
      "Sellers Sellers them\n",
      "Amazon Amazon\n",
      "sellers sellers\n",
      "Kelsey Skea Kelsey Skea\n",
      "Editorial Director Editorial Director\n",
      "Amazon Crossing Kids Amazon Crossing Kids\n",
      "Marilyn Brigham Marilyn Brigham\n",
      "Gabriella Page-Fort Gabriella Page-Fort\n",
      "Amazon Amazon\n",
      "NASDAQ: AMZN NASDAQ: AMZN\n",
      "Amazon Amazon leader\n",
      "Dec. 26, 2019 Dec. 26, 2019 today\n",
      "Amazon Amazon We\n",
      "Deltona Deltona\n",
      "Florida Florida\n",
      "Employees Employees\n",
      "Since 2013 Since 2013\n",
      "employees employees\n",
      "Nov. 20, 2019 Nov. 20, 2019 today\n",
      "Amazon Amazon\n",
      "Florida Florida\n",
      "Employees Employees\n",
      "associates associates\n",
      "Amazon Amazon company\n",
      "workforce workforce\n",
      "area area Oregon\n",
      "since 2010 since 2010\n",
      "employees employees\n",
      "Aug. 21, 2019 Aug. 21, 2019 today\n",
      "Portland Tech Hub Portland Tech Hub Portland\n",
      "Since 2010 Since 2010\n",
      "Oregon Oregon state\n",
      "AWS AWS Amazon Web Services, Inc. Amazon Web Services\n",
      "Spain Spain\n",
      "Amazon Amazon\n",
      "Ring Ring\n",
      "Apr. 12, 2018 Apr. 12, 2018\n",
      "neighborhoods neighborhoods\n",
      "companies companies\n",
      "home home\n",
      "site site fulfillment center\n",
      "Bondurant Bondurant\n",
      "Feb. 6, 2020 Feb. 6, 2020 today\n",
      "Amazon.com, Inc. Amazon.com, Inc. We Amazon\n",
      "East Polk County East Polk County\n",
      "employees employees\n",
      "Iowans Iowans\n",
      "Bondurant Bondurant City\n",
      "community community\n",
      "Eastern Polk Eastern Polk\n",
      "Amazonians Amazonians\n",
      "Amazon Amazon\n",
      "$2 Million $2 Million\n",
      "Seattle Seattle\n",
      "Nonprofits Nonprofits\n",
      "Washington State Washington State Washington\n",
      "Washington STEM Washington STEM\n",
      "Pacific Science Center Pacific Science Center PacSci\n",
      "Jan. 23, 2020 Jan. 23, 2020 today\n",
      "nonprofit nonprofit organizations\n",
      "Washington State Washington State Washington\n",
      "Amazon Amazon\n",
      "we we Science On Wheels\n",
      "Amazon Amazon\n",
      "Tennessee Tennessee state\n",
      "Amazon Future Engineer Amazon Future Engineer\n",
      "Amazon Amazon\n",
      "organizations organizations\n",
      "country country\n",
      "Nashville Nashville\n",
      "Amazon Amazon company\n",
      "workforce workforce\n",
      "Tomorrow Tomorrow September 17th\n",
      "Illinois—candidates Illinois—candidates\n",
      "amazon.jobs/careerday amazon.jobs/careerday\n",
      "Illinois Illinois state\n",
      "since 2010 since 2010\n",
      "Sep. 16, 2019 Sep. 16, 2019 today\n",
      "talent talent\n",
      "talent talent\n",
      "Illinois Illinois state\n",
      "since 2010 since 2010 Since 2010\n",
      "company company Amazon\n",
      "local local\n",
      "workforce workforce\n",
      "in the coming years in the coming years\n",
      "since 2010 since 2010\n",
      "infrastructure infrastructure\n",
      "Feb. 4, 2020 Feb. 4, 2020 today\n",
      "workforce workforce\n",
      "employees employees\n",
      "talent talent\n",
      "community community\n",
      "SEATTLE SEATTLE Toronto\n",
      "December 18, 2018 December 18, 2018 today\n",
      "Amazon Amazon company\n",
      "jobs jobs\n",
      "talent talent\n",
      "people people\n",
      "Canada Canada Canadian\n",
      "this year this year\n",
      "Since 2011 Since 2011\n",
      "CAD $3 billion CAD $3 billion\n",
      "To date To date\n",
      "Amazonians Amazonians\n",
      "Candidates Candidates\n",
      "roles roles\n",
      "Amazon Amazon\n",
      "$15 Billion $15 Billion\n",
      "in 2019 in 2019 year\n",
      "businesses businesses\n",
      "Feb. 4, 2020 Feb. 4, 2020 today\n",
      "last year last year\n",
      "billions of dollars billions of dollars\n",
      "April 27, 1998 April 27, 1998\n",
      "Amazon.com, Inc. Amazon.com, Inc. it company\n",
      "Internet Movie Database Ltd. Internet Movie Database Ltd. Internet Movie Database\n",
      "European European\n",
      "United Kingdom United Kingdom U.K.\n",
      "charges charges $55 million\n",
      "Amazon Future Engineer Amazon Future Engineer Amazon\n",
      "organizations organizations\n",
      "country country\n",
      "Amazon Amazon\n",
      "investors investors\n",
      "Critical Role Critical Role\n",
      "Amazon army Amazon army\n",
      "they they Vox Machina Critical Role\n",
      "Emon Emon\n",
      "monster monster\n",
      "forces forces\n",
      "Titmouse Titmouse company\n",
      "people people\n",
      "ProSiebenSat.1 ProSiebenSat.1 company www.prosiebensat1.com\n",
      "In 2018 In 2018\n",
      "Amazon Amazon Amazon.com, Inc.\n",
      "people people\n",
      "Washington State Washington State state\n",
      "associates associates\n",
      "SEATTLE SEATTLE\n",
      "Jul. 20, 2018 Jul. 20, 2018 today\n",
      "from 2011-2017 from 2011-2017\n",
      "Nov. 18, 2019 Nov. 18, 2019 today\n",
      "Amazon.com Amazon.com Amazon\n",
      "Amazonians Amazonians\n",
      "candidates candidates\n",
      "Amazon Amazon company\n",
      "Nevada Nevada state\n",
      "since 2011 since 2011\n",
      "Nov. 14, 2019 Nov. 14, 2019 today\n",
      "workforce workforce\n",
      "Tami Dennis Tami Dennis\n",
      "food bank food bank\n",
      "Three Square Food Bank Three Square Food Bank\n",
      "Nevada Nevada\n",
      "today today\n",
      "since 2011 since 2011\n",
      "customers customers customers’\n",
      "AWS AWS\n",
      "Amazon Amazon it\n",
      "city city\n",
      "HQ2 HQ2\n",
      "U.S. U.S.\n",
      "North America North America\n",
      "infrastructure infrastructure\n",
      "tens of billions of dollars tens of billions of dollars\n",
      "community community\n",
      "Over the past five years Over the past five years\n",
      "Amazon Amazon Amazon.com, Inc.\n",
      "currently currently\n",
      "people people\n",
      "associates associates\n",
      "Aug. 19, 2019 Aug. 19, 2019 today\n",
      "Amazon Amazon company\n",
      "Since 2010 Since 2010\n",
      "U.S. U.S.\n",
      "merger merger all-cash transaction partnership\n",
      "acquire acquire\n",
      "remain remain\n",
      "transaction transaction\n",
      "merger merger transaction\n",
      "financing financing\n",
      "Merger Merger\n",
      "hire hire\n",
      "transaction transaction TRANSACTION\n",
      "TRANSACTION TRANSACTION\n",
      "acquisition acquisition transaction\n",
      "transaction transaction\n",
      "acquisition acquisition\n",
      "integrate integrate\n",
      "grow grow\n",
      "create create\n",
      "jobs jobs\n",
      "hires hires\n",
      "will will\n",
      "remain remain\n",
      "stay stay\n",
      "acquisition acquisition transaction\n",
      "create create hiring join expansion welcome bring bringing creating\n",
      "donate donate donating fund donation funding\n",
      "job opportunities job opportunities\n",
      "investment investment\n",
      "signed up signed up\n",
      "acquisitions acquisitions\n",
      "invested invested\n",
      "funded funded\n",
      "received received funding invested\n",
      "invest invest\n",
      "invested invested\n",
      "created created\n",
      "joined joined\n",
      "invest invest\n",
      "Investments Investments\n",
      "investments investments\n",
      "applications applications filling hired job\n",
      "hiring hiring\n",
      "hiring hiring\n",
      "work work\n",
      "Donating Donating\n",
      "agreement agreement acquire\n",
      "merger merger\n",
      "transaction transaction\n",
      "transaction transaction\n",
      "applying applying\n",
      "apply apply\n",
      "created created\n",
      "investing investing\n",
      "selling selling\n",
      "hire hire\n",
      "invested invested\n",
      "direct hires direct hires\n",
      "creating creating jobs\n",
      "gifts gifts donation investment\n",
      "gift gift donations donate donation\n",
      "donate donate\n",
      "creating creating\n",
      "work work\n",
      "create create\n",
      "investments investments\n",
      "invest invest\n",
      "move into move into creating joining job creation\n",
      "jobs jobs\n",
      "employ employ\n",
      "investment investment\n",
      "brought brought\n",
      "invested invested\n",
      "investments investments\n",
      "created created\n",
      "direct hires direct hires\n",
      "creating creating create\n",
      "investment investment\n",
      "invested invested\n",
      "hiring hiring\n",
      "invested invested investments\n",
      "created created\n",
      "hires hires\n",
      "steps steps\n",
      "investment investment\n",
      "investment investment\n",
      "investments investments\n",
      "investments investments\n",
      "investments investments\n",
      "investments investments\n",
      "investments investments\n",
      "investment investment\n",
      "investments investments\n",
      "invested invested\n",
      "raise raise\n",
      "investment investment\n",
      "created created expansion create job creation joining adding expand\n",
      "invested invested\n",
      "invest invest\n",
      "investment investment\n",
      "investments investments\n",
      "invested invested investments\n",
      "invested invested\n",
      "hiring hiring\n",
      "partnership partnership\n",
      "support support\n",
      "investment investment\n",
      "funding funding\n",
      "hiring hiring\n",
      "investment investment\n",
      "investment investment\n",
      "invest invest\n",
      "hiring hiring\n",
      "job openings job openings\n",
      "job job\n",
      "jobs jobs\n",
      "created created working\n",
      "jobs jobs\n",
      "investments investments\n",
      "invested invested\n",
      "invested invested\n",
      "hired hired\n",
      "hired hired\n",
      "join join\n",
      "working working\n",
      "created created\n",
      "jobs jobs\n",
      "reinvest reinvest\n",
      "job creation job creation\n",
      "listing listing\n",
      "hire hire\n",
      "created created jobs\n",
      "investment investment\n",
      "investment investment\n",
      "investment investment\n",
      "trading trading\n",
      "invests invests\n",
      "investments investments\n",
      "acquisitions acquisitions\n",
      "transactions transactions\n",
      "filings filings\n",
      "investment investment\n",
      "invest invest\n",
      "former former\n",
      "leave leave\n",
      "hired hired\n",
      "invested invested\n",
      "created created jobs\n",
      "investment investment\n",
      "create create\n",
      "jobs jobs\n",
      "hire hire\n",
      "joining joining\n",
      "hire hire\n",
      "create create\n",
      "jobs jobs\n",
      "deal deal acquired\n",
      "will serve will serve\n",
      "investment investment\n",
      "listed listed\n",
      "create create working\n",
      "employs employs\n",
      "employs employs\n",
      "employs employs hiring\n",
      "create create\n",
      "investments investments\n",
      "combining combining Together\n",
      "combination combination\n",
      "hire hire\n",
      "hire hire\n",
      "invest invest\n",
      "acquisition acquisition acquired\n",
      "sell sell\n",
      "selling selling\n",
      "hire hire\n",
      "creating creating\n",
      "created created jobs\n",
      "investment investment\n",
      "partnership partnership\n",
      "investment investment\n",
      "investment investment\n",
      "employing employing\n",
      "trading trading\n",
      "trading trading\n",
      "investment investment\n",
      "acquire acquire\n",
      "combination combination\n",
      "investments investments\n",
      "invest invest\n",
      "acquired acquired\n",
      "generated generated\n",
      "investment investment\n",
      "donated donated\n",
      "investment investment\n",
      "jobs jobs\n",
      "investment investment\n",
      "investment investment\n",
      "investment investment\n",
      "donated donated\n",
      "created created employs expansion create\n",
      "invested invested investment\n",
      "created created\n",
      "investment investment\n",
      "job creation job creation\n",
      "invested invested investments\n",
      "hires hires\n",
      "invested invested\n",
      "investment investment\n",
      "hire hire\n",
      "investment investment\n",
      "donated donated\n",
      "investment investment invest Closed Loop Fund investing\n",
      "jobs jobs\n",
      "invest invest\n",
      "investment investment\n",
      "joined joined\n",
      "program program\n",
      "selling selling loan lends\n",
      "infusion infusion\n",
      "loans loans\n",
      "loans loans Lending buy loan\n",
      "selling selling\n",
      "grow grow job creation create\n",
      "hiring hiring\n",
      "investment investment\n",
      "investment investment invest bring\n",
      "investment investment invested investments\n",
      "RFP RFP\n",
      "investments investments\n",
      "hire hire move\n",
      "continue working continue working\n",
      "invests invests\n",
      "investments investments\n",
      "acquisitions acquisitions\n",
      "investment investment\n",
      "donated donated\n",
      "acquisition acquisition\n",
      "create create\n",
      "jobs jobs\n",
      "invest invest\n",
      "joining joining\n",
      "out out\n",
      "investment investment\n",
      "jobs jobs\n",
      "created created\n",
      "investment investment\n",
      "invested invested\n",
      "created created\n",
      "sign sign\n",
      "acquired acquired deal\n",
      "acquire acquire transaction deal\n",
      "cash investment cash investment deal investment\n",
      "staff reductions staff reductions\n",
      "trading trading\n",
      "acquisitions acquisitions\n",
      "indebtedness indebtedness\n",
      "acquired acquired\n",
      "acquisition acquisition\n",
      "merger merger\n",
      "acquire acquire transaction\n",
      "transaction transaction\n",
      "employing employing\n",
      "creating creating create add join\n",
      "investing investing investments partner\n",
      "jobs jobs\n",
      "train train signing up\n",
      "adding adding create jobs working\n",
      "job job\n",
      "invests invests\n",
      "invests invests\n",
      "investments investments\n",
      "jobs jobs\n",
      "employs employs\n",
      "investment investment\n",
      "invest invest\n",
      "create create\n",
      "invest invest investments\n",
      "created created jobs\n",
      "job creation job creation\n",
      "jobs jobs\n",
      "invests invests investment\n",
      "invest invest\n",
      "investment investment\n",
      "donated donated\n",
      "creating creating work bring\n",
      "investment investment\n",
      "employment employment\n",
      "move move\n",
      "created created\n",
      "invested invested\n",
      "create create creating\n",
      "created created\n",
      "invest invest\n",
      "employing employing\n",
      "investment investment\n",
      "investment investment\n",
      "invested invested\n",
      "investments investments\n",
      "hires hires\n",
      "raised raised fund\n",
      "invested invested\n",
      "combined combined\n",
      "investments investments\n",
      "invest invest investment\n",
      "jobs jobs\n",
      "investments investments\n",
      "investments investments\n",
      "Investments Investments investments\n",
      "investments investments\n",
      "invest invest\n",
      "investments investments\n",
      "investment investment\n",
      "create create\n",
      "investments investments\n",
      "investments investments\n",
      "investing investing\n",
      "investment investment\n",
      "create create\n",
      "investments investments\n",
      "investments investments\n",
      "employ employ\n",
      "employing employing\n",
      "donate donate supporting\n",
      "acquisitions acquisitions\n",
      "invest invest\n",
      "hire hire\n",
      "invested invested\n",
      "created created\n",
      "jobs jobs\n",
      "acquisition acquisition agreement acquire\n",
      "acquisitions acquisitions\n",
      "sell sell\n",
      "invested invested\n",
      "combinations combinations\n",
      "acquisition acquisition\n",
      "investment investment\n",
      "investing investing\n",
      "join join\n",
      "Fulfillment Fulfillment\n",
      "created created\n",
      "created created\n",
      "Lending Lending\n",
      "serve serve\n",
      "combined combined\n",
      "investments investments\n",
      "creating creating\n",
      "investments investments\n",
      "investments investments\n",
      "create create creating\n",
      "investment investment\n",
      "invested invested\n",
      "created created\n",
      "investment investment\n",
      "investing investing\n",
      "employment employment\n",
      "jobs jobs\n",
      "working working\n",
      "create create jobs\n",
      "employs employs\n",
      "investment investment\n",
      "working working\n",
      "double double expansion create adding\n",
      "created created\n",
      "invested invested\n",
      "employs employs create\n",
      "hiring hiring\n",
      "invest invest\n",
      "jobs jobs\n",
      "invested invested\n",
      "investments investments\n",
      "hires hires\n",
      "investment investment\n",
      "invest invest\n",
      "investment investment\n",
      "investments investments\n",
      "Acquisition—Now Acquisition—Now acquisition\n",
      "investment investment\n",
      "create create creating work be employed\n",
      "investment investment\n",
      "welcome welcome work creating\n",
      "jobs jobs\n",
      "investment investment project It investments\n",
      "employment employment\n",
      "invest invest\n",
      "move move\n",
      "Donates Donates donating donation\n",
      "investment investment\n",
      "fund fund\n",
      "investment investment\n",
      "donated donated\n",
      "bring bring\n",
      "investment investment\n",
      "job creation job creation\n",
      "donated donated\n",
      "double double register created expansion create job creation apply doubling attract\n",
      "invested invested\n",
      "hiring hiring\n",
      "fill fill\n",
      "created created\n",
      "invested invested\n",
      "invested invested investments\n",
      "hires hires\n",
      "double double doubling\n",
      "created created create\n",
      "invested invested\n",
      "expansion expansion create double\n",
      "created created\n",
      "hire hire\n",
      "invest invest\n",
      "Investing Investing invested investments\n",
      "creating creating\n",
      "giving giving\n",
      "created created\n",
      "hires hires\n",
      "expansion expansion create\n",
      "fill fill\n",
      "investment investment\n",
      "created created\n",
      "retain retain\n",
      "employ employ\n",
      "invest invest\n",
      "create create\n",
      "invested invested\n",
      "job-retraining job-retraining\n",
      "working working\n",
      "applying applying\n",
      "Invested Invested investments\n",
      "investment investment\n",
      "invested invested investment\n",
      "hiring hiring\n",
      "jobs jobs\n",
      "investments investments\n",
      "acquired acquired acquisitions acquisition purchase transactions\n",
      "transactions transactions\n",
      "selling selling\n",
      "investment investment\n",
      "donated donated\n",
      "trading trading\n",
      "market trading market trading\n",
      "market trading market trading\n",
      "trading trading\n",
      "Kickstarter Kickstarter\n",
      "employed employed\n",
      "invested invested\n",
      "employ employ creating employs\n",
      "jobs jobs\n",
      "invested invested\n",
      "jobs jobs\n",
      "job job\n",
      "investment investment\n",
      "employment employment\n",
      "working working\n",
      "create create\n",
      "job job\n",
      "move move\n",
      "working working\n",
      "created created opening expansion create\n",
      "invested invested\n",
      "create create\n",
      "invest invest\n",
      "investment investment\n",
      "donation donation\n",
      "created created\n",
      "invested invested investments\n",
      "hires hires\n",
      "investments investments\n",
      "investments investments\n",
      "create create hiring\n",
      "invest invest investment\n",
      "investment investment\n",
      "job creation job creation\n",
      "hiring hiring\n",
      "investment investment\n",
      "invested invested\n",
      "employs employs employ creating grow employment\n",
      "jobs jobs\n",
      "job job\n",
      "jobs jobs\n",
      "joining joining\n",
      "invested invested\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import helpers\n",
    "\n",
    "def make_entity_embedding(toks):\n",
    "    text=\" \".join(toks)\n",
    "    print(text)\n",
    "    return helpers.embedding_string(helpers.make_embedding(text))\n",
    "\n",
    "taxonomy_labels=['prefLabel', 'altLabels', 'broaders']\n",
    "struct_labels=['label', 'labels', 'ulabels', 'typeLabels', 'seeAlsoLabels', 'seeAlsoTypeLabels']\n",
    "extract_labels=['label', 'labels']\n",
    "entities_to_embed={\n",
    "    'orgs': struct_labels, \n",
    "    'persons': struct_labels, \n",
    "    'products': struct_labels,  \n",
    "    'industries': struct_labels, \n",
    "    'locations': struct_labels,  \n",
    "    'services': struct_labels, \n",
    "    'taxonomy_concepts': taxonomy_labels, \n",
    "    'extractions': extract_labels\n",
    "}\n",
    "\n",
    "with open(\"graphdata/lpg/entity_embeddings.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"embedding:vector\"])\n",
    "    \n",
    "    for ee in entities_to_embed:\n",
    "        print(ee)\n",
    "        with open(f'graphdata/lpg/{ee}.csv', newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            posof={}\n",
    "            for index, row in enumerate(csvreader):\n",
    "                if index==0:\n",
    "                    for rindex, r in enumerate(row):\n",
    "                        posof[r] = rindex\n",
    "                    print(posof)\n",
    "                else:\n",
    "                    _id = row[0]\n",
    "                    _label=row[1]\n",
    "                    candidates=[]\n",
    "                    for col in entities_to_embed[ee]:\n",
    "                        candidates += row[posof[col]].split(helpers.CELL_DELIM)\n",
    "                    embedding_text=make_entity_embedding(candidates)\n",
    "                    writer.writerow([_id, _label, embedding_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d705bf-484d-4acf-b2d8-e20bfd9933c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
