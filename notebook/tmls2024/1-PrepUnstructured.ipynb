{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2d9b7-5ab8-4f26-b3b0-bd0c4786cf39",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Ask the Graph\n",
    "# Notebook 1: Prep Unstructured Data\n",
    "\n",
    "In this notebook we prepare unstructured data, a set of press releases about organizations. This data complements the structured organization data we prepared in notebook 0. \n",
    "\n",
    "You DO NOT need to run this notebook. The notebook produces output that is already prepared for you and available in a public bucket. But you may wish to review how we prepared that data. In that case, follow along with the logic below.\n",
    "\n",
    "Here is the input, sourced from s3://aws-neptune-customer-samples/tmls2024/source/. We download a local copy to the source folder:\n",
    "\n",
    "- comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl: Press releases, including title, keywords, and text. This file also contains entities and events extracted from press releases using Amazon Comprehend.\n",
    "\n",
    "We produce the following output locally:\n",
    "- graphdata/lpg - CSV files of embeddings and extracted entities to bulk-load as LPG data to Neptune. \n",
    "- graphdata/rdf - Turtle files to bulk-load to Neptune as RDF data. (TODO - is this needed?)\n",
    "- documents - Press releases as .txt files \n",
    "- chunks - Chunks of press releases as .txt files\n",
    "- summaries - Summaries of press releases as .txt files\n",
    "\n",
    "That same data is available in:\n",
    "s3://aws-neptune-customer-samples/tmls2024/graphdata/\n",
    "s3://aws-neptune-customer-samples/tmls2024/documents/\n",
    "s3://aws-neptune-customer-samples/tmls2024/chunks/\n",
    "s3://aws-neptune-customer-samples/tmls2024/summaries/\n",
    "\n",
    "Specific files that we produce are the following. LPG files in graphdata/lpg are:\n",
    "\n",
    "- documents.csv - Press release documents and their summary as an embedding.\n",
    "- summaries.csv - References the documents from documents.csv but adding their embedding from summary.\n",
    "- extractions.csv - Entities and events extracted from press releases.\n",
    "- extraction_rels.csv - Links between documents and their extracted entities and events. \n",
    "- resolved_entities.csv - Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "- resolution_links.csv -  Resolution matches that can be linked from extracted node to structured node.\n",
    "- chunks.csv - Document chunks and their embeddings.\n",
    "- chunk2doc.csv -Link chunks to docs.\n",
    "- entity_embeddings.csv - Embeddings of entities for searchability\n",
    "\n",
    "RDF in graphdata/rdf are:\n",
    "- documents.ttl - Press release documents.\n",
    "- extractions.ttl - Entities and events extracted from press releases.\n",
    "- resolved_entities.ttl -Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "\n",
    "TODO mention embeddings are kept in OpenSearch for RDF..\n",
    "\n",
    "These files are also maintained in:\n",
    "\n",
    "- s3://aws-neptune-customer-samples/tmls2024/graphdata/\n",
    "- s3://aws-neptune-customer-samples/tmls2024/documents/\n",
    "- s3://aws-neptune-customer-samples/tmls2024/chunks/\n",
    "- s3://aws-neptune-customer-samples/tmls2024/summaries/\n",
    "\n",
    "TODO - data model, including what we've built so far\n",
    "\n",
    "In the next notebook, we load prepared data into Neptune for query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0abb6f4-313a-45cd-95c7-02a7dbbba2f6",
   "metadata": {},
   "source": [
    "## Install a few dependencies\n",
    "We use Langchain very lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1990855-7e68-4173-bc4e-f0bfa8d2844d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters langchain-community unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18867a0",
   "metadata": {},
   "source": [
    "## Get source data and create output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267563be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://aws-neptune-customer-samples-us-east-1/tmls2024/source/ source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db952cc7-326d-4264-ada2-60d88eb676fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "mkdir -p graphdata graphdata/rdf graphdata/lpg summaries chunks documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf1f3e-7db2-4c0f-82f9-cb71d1c1fa81",
   "metadata": {},
   "source": [
    "## Build press release documents\n",
    "\n",
    "We have a set of press releases. These documents contain useful information that we would like to link to our base organization KG. \n",
    "\n",
    "In this set we build those documents.\n",
    "\n",
    "The result is a CSV file written to the *graphdata* folder:\n",
    "\n",
    "- lpg/documents.csv \n",
    "- rdf/documents.ttl\n",
    "\n",
    "Also *.txt* files are written to the *documents* folder. The name of each file is *docid*.txt, where *docid* is the vertex ID of the document in the graph.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e8978-41ac-439d-ae38-96cafc37f00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "def make_document_uri(docid):\n",
    "    return helpers.make_uri(f\"Document/{docid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc326317-220f-41c3-9040-facad75cbea2",
   "metadata": {},
   "source": [
    "## The RAG-prep part: Summarize docs Chunk, embed, and extract files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887b1e4-bbab-4a56-8330-2e95940f239e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, json\n",
    "import helpers\n",
    "from rdflib import Graph, Literal, RDF, RDFS, URIRef, XSD, OWL, BNode, DC, SKOS\n",
    "\n",
    "prdocs = []\n",
    "\n",
    "# Open the JSONL file that contains the documents plus the Comprehend extraction. \n",
    "jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "for index, row in jsonObj.iterrows():\n",
    "        \n",
    "    # extract metadata about current press release\n",
    "    metadata=row['metadata']\n",
    "    m_keywords=metadata['keywords']\n",
    "    m_title=metadata['title']\n",
    "    m_doc=metadata['document_id']\n",
    "\n",
    "    # write text to a file for chunking/embedding later\n",
    "    with open(f\"documents/{m_doc}.txt\", \"w\") as f:\n",
    "        f.write(row['raw_text'])\n",
    "    \n",
    "    prdocs.append([make_document_uri(m_doc), \"Document\", m_doc, m_title, m_keywords])\n",
    "\n",
    "# write docs to CSV for LPG\n",
    "with open('graphdata/lpg/documents.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"docuuid\", \"title\", \"keywords\"])\n",
    "    for p in prdocs:\n",
    "        writer.writerow(p)\n",
    "\n",
    "# write docs for RDF\n",
    "rdf_file = helpers.rdf_open()\n",
    "for p in prdocs:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), RDF.type, helpers.make_uri(\"Document\"))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), DC.title, Literal(p[3]))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), helpers.make_uri(\"uuid\"), Literal(p[2]))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p[0]), helpers.make_uri(\"keywords\"), Literal(p[4]))\n",
    "    \n",
    "        \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/documents.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efa70d-9690-4f45-8d83-f01f8cdfd46b",
   "metadata": {},
   "source": [
    "## Build Comprehend extraction results\n",
    "\n",
    "We ran Amazon Comprehend to extract entities and events from the press releases. \n",
    "\n",
    "In this step, we build those extractions and link them to documents.\n",
    "\n",
    "Specific files that we produce are the followin:\n",
    "\n",
    "- extracted_entities.csv\n",
    "- extracted_events.csv \n",
    "- extraction.ttl - Entities and events extracted from press releases.\n",
    "\n",
    "For more on this approach, see blog post https://aws.amazon.com/blogs/database/building-a-knowledge-graph-in-amazon-neptune-using-amazon-comprehend-events/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54911735-f04b-421d-a08b-f8ddb5396e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will filter out names referring to each entity with less than 0.95 group certainty.\n",
    "# You can change this threshold to be lower if you are tolerant of less certain values in your data set.\n",
    "groupThreshold = 0.95\n",
    "\n",
    "entities=[]\n",
    "events=[]\n",
    "ee_rels=[]\n",
    "de_rels=[]\n",
    "distinct_roles={}\n",
    "\n",
    "def strip_x_id(rawid):\n",
    "    return \"_\".join(rawid.split()).lower() # replace all whitespace with underscores\n",
    "\n",
    "# Open the JSONL file that contains the documents plus the Comprehend extraction. \n",
    "jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "for index, row in jsonObj.iterrows():\n",
    "        \n",
    "    # extract metadata about current press release\n",
    "    metadata=row['metadata']\n",
    "    m_doc=metadata['document_id']\n",
    "    \n",
    "    # Comprehend Events references entities it refers to by index, so we need to retain the ordered list of entities\n",
    "    # within the document\n",
    "    annotations = row['annotations']\n",
    "    for entity in annotations[\"Entities\"]:\n",
    "        primary_name = entity[\"Mentions\"][0][\"Text\"]\n",
    "        entity_type =entity[\"Mentions\"][0][\"Type\"]\n",
    "        entity_local=strip_x_id(f\"{entity_type}_{primary_name}\")\n",
    "        entity_id= helpers.make_uri(f\"ExtractedEntity/{entity_local}\")\n",
    "        names=[]\n",
    "        for mention in entity[\"Mentions\"]:\n",
    "            if (mention[\"GroupScore\"] >= groupThreshold):\n",
    "                if not(mention[\"Text\"] in names):\n",
    "                        names.append(mention[\"Text\"])\n",
    "        entities.append({\n",
    "            '~id': entity_id,\n",
    "            'local_id': entity_local,\n",
    "            '~label': 'ExtractedEntity',\n",
    "            'label': primary_name,\n",
    "            'labels': names,\n",
    "            'type': entity_type\n",
    "        })\n",
    "        \n",
    "    for event in annotations[\"Events\"]:\n",
    "        primary_name=event[\"Triggers\"][0][\"Text\"]\n",
    "        event_type=event[\"Type\"]\n",
    "        offset_for_id=str(event[\"Triggers\"][0][\"BeginOffset\"])\n",
    "        event_local=strip_x_id(f\"{m_doc}_{event_type}_{primary_name}{offset_for_id}\")\n",
    "        event_id=helpers.make_uri(f\"ExtractedEvent/{event_local}\")\n",
    "        names=[]\n",
    "        for trigger in event[\"Triggers\"]:\n",
    "            if not(trigger[\"Text\"] in names):\n",
    "                names.append(trigger[\"Text\"])\n",
    "\n",
    "        events.append({\n",
    "            '~id': event_id,\n",
    "            'local_id': event_local,\n",
    "            '~label': 'ExtractedEvent',\n",
    "            'label': primary_name,\n",
    "            'labels': names,\n",
    "            'type': event_type\n",
    "        })\n",
    "\n",
    "        # add edges between the event node and the entity node, \n",
    "        # annotated with a label describing the Comprehend Event role assigned to the entity in the event.\n",
    "        for argument in event[\"Arguments\"]:\n",
    "            from_id=event_id\n",
    "            from_local=event_local\n",
    "            to_ent=entities[argument[\"EntityIndex\"]]\n",
    "            to_id=to_ent['~id']\n",
    "            to_local=to_ent['local_id']\n",
    "            edge_type=argument[\"Role\"]\n",
    "            ee_local=strip_x_id(f\"{from_local}_{to_local}_{edge_type}\")\n",
    "            ee_edge_id=helpers.make_uri(f\"ExtractedEventToEntity/{ee_local}\")\n",
    "            role= argument[\"Role\"]\n",
    "            role_uri=helpers.make_uri(f\"ExtractedRole/{role}\")\n",
    "            distinct_roles[role_uri]=role\n",
    "            ee_rels.append({\n",
    "                '~id': ee_edge_id,\n",
    "                '~label': \"eventHasEntity\",\n",
    "                'type': event[\"Type\"],\n",
    "                '~from': from_id,\n",
    "                '~to': to_id,\n",
    "                \"role\":  role\n",
    "            })\n",
    "        \n",
    "        # add an edge between the document and the event nodes\n",
    "        document_id = make_document_uri(m_doc)\n",
    "        de_local=strip_x_id(f\"{m_doc}_{event_local}\")\n",
    "        de_edge_id=helpers.make_uri(f\"DocumentToExtractedEvent/{de_local}\")\n",
    "        de_rels.append({\n",
    "            '~id': de_edge_id,\n",
    "            '~label': \"documentHasEvent\",\n",
    "            '~from': document_id,\n",
    "            '~to': event_id,\n",
    "            'role': \"\" \n",
    "        })\n",
    "        \n",
    "\n",
    "# write docs to CSV for LPG\n",
    "with open('graphdata/lpg/extractions.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"label\", \"labels\", \"type\"])\n",
    "    for p in entities + events:\n",
    "        writer.writerow([p['~id'], p['~label'], p['label'], helpers.get_delim_string(p, 'labels'), p['type']])\n",
    "\n",
    "with open('graphdata/lpg/extraction_rels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~from\", \"~to\", \"label\", \"role\"])\n",
    "    for p in ee_rels + de_rels:\n",
    "        writer.writerow([p['~id'], p['~from'], p['~to'], p['~label'], p['role']])\n",
    "        \n",
    "# write docs for RDF\n",
    "rdf_file = helpers.rdf_open()\n",
    "    \n",
    "for d in distinct_roles:\n",
    "    helpers.rdf_write(rdf_file, URIRef(d), RDF.type, helpers.make_uri(\"ExtractionRole\"))\n",
    "    helpers.rdf_write(rdf_file, URIRef(d), RDFS.label, Literal(distinct_roles[d]))\n",
    "\n",
    "for p in entities + events:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDF.type, helpers.make_uri(\"Extraction\"))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDF.type, helpers.make_uri(p['~label']))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDF.type, helpers.make_uri(p['type']))\n",
    "\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), RDFS.label, Literal(p['label']))\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~id']), helpers.make_uri(\"xtype\"), Literal(p['type']))\n",
    "    for l in p['labels']:\n",
    "        helpers.rdf_write(rdf_file, URIRef(p['~id']), SKOS.altLabel, Literal(l))\n",
    "\n",
    "for p in de_rels:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~from']),helpers.make_uri(p['~label']), URIRef(p['~to']))\n",
    "        \n",
    "for p in ee_rels:\n",
    "    helpers.rdf_write(rdf_file, URIRef(p['~from']),helpers.make_uri(p['role']), URIRef(p['~to']))\n",
    "    \n",
    "        \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/extactions.ttl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf66e3d-6035-4b74-b440-9ad17ea37cb7",
   "metadata": {},
   "source": [
    "### Entity Resolution, LLM Style\n",
    "Now let's try to link entities mentioned in Comprehend output to orgs, persons, industries, locations, products, and services in the graph. \n",
    "\n",
    "We'll get creative! Let's ask the LLM to find well-known URIs and alternate names for the entities and events discovered above.\n",
    "\n",
    "- resolved_entities.csv - Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "- resolution_links.csv -  Resolution matches that can be linked from extracted node to structured node.\n",
    "\n",
    "RDF in graphdata/rdf are:\n",
    "- resolved_entities.ttl -Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d3c9e-9123-4b0e-bca1-35d821abb015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track entities to resolve\n",
    "er_entities={}\n",
    "for e in entities:\n",
    "    if e['type'] in ['PERSON', 'PERSON_TITLE', 'LOCATION', 'ORGANIZATION', 'STOCK_CODE']:\n",
    "        er_entities[e['~id']] = {'~id': e['~id'], 'label': e['label'], 'type': e['type']}\n",
    "\n",
    "count=len(er_entities)\n",
    "progress=0\n",
    "for e in er_entities:\n",
    "    if progress % 10 == 0:\n",
    "        print(str(progress))\n",
    "    er_entities[e]['resolution']=helpers.resolve_entities(er_entities[e]['label'])\n",
    "    progress += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b607cc-0ad2-419b-9d3d-a61b54b7787a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('saved_er.json', 'w') as saved_er: \n",
    "    saved_er.write(json.dumps(er_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12fd22-f35f-4151-9541-945151793c2f",
   "metadata": {},
   "source": [
    "## Save these entities and see if we can resolve them once they land in the graph\n",
    "\n",
    "- resolved_entities.csv - Link extracted entities to existing orgs, persons, industries, locations, products, and services in the graph.\n",
    "\n",
    "- extractions.ttl - Entities and events extracted from press releases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3dc69-50b8-4048-b64b-ced4db5b38cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_uri(uri):\n",
    "    return uri.startswith(\"http://\") or uri.startswith(\"https://\")\n",
    "\n",
    "resies=[]\n",
    "\n",
    "for eid in er_entities:\n",
    "    for index, r in enumerate(er_entities[eid]['resolution']):\n",
    "        resid=helpers.make_uri(f\"{eid}_res{index}\")\n",
    "        resies.append({\n",
    "            '~id': resid, \n",
    "            '~label': \"ExtractedEntityAltTerm\", \n",
    "            'label': r, \n",
    "            'is_uri': is_uri(r),\n",
    "            'entity_id': eid,\n",
    "            'edge_id': helpers.make_uri(f\"{eid}_res{index}_link\"),\n",
    "            'edge_type': \"LinkedAtlTerm\"\n",
    "        })\n",
    "        \n",
    "# write docs to CSV for LPG\n",
    "with open('graphdata/lpg/resolved_entities.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"label\"])\n",
    "    for e in resies:\n",
    "        writer.writerow([e['~id'], e['~label'], e['label']])\n",
    "\n",
    "with open('graphdata/lpg/extraction_links.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~from\", \"~to\", \"label\"])\n",
    "    for e in resies:\n",
    "        if e['is_uri']:\n",
    "            writer.writerow([e['edge_id'], e['entity_id'], e['label'], e['edge_type']])\n",
    "        \n",
    "# write docs for RDF\n",
    "rdf_file = helpers.rdf_open()\n",
    "\n",
    "for e in resies:\n",
    "    helpers.rdf_write(rdf_file, URIRef(e['~id']), RDF.type, helpers.make_uri(e['~label']))\n",
    "    helpers.rdf_write(rdf_file, URIRef(e['~id']), RDFS.label, Literal(e['label']))\n",
    "    if e['is_uri']:\n",
    "        helpers.rdf_write(rdf_file, URIRef(e['entity_id']), helpers.make_uri(e['edge_type']), URIRef(e['~label']))\n",
    "        \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/resolved_entities.ttl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e4cfa-ec0b-48ba-adec-549bf5675ee3",
   "metadata": {},
   "source": [
    "## Create document summaries\n",
    "Ask LLM to summarize each press release document. \n",
    "\n",
    "Save summaries as .txt files in summaries folder.\n",
    "\n",
    "Also create CSV file keeping embeddings of each summary:\n",
    "\n",
    "- summaries.csv - References the documents from documents.csv but adding their embedding from summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2e5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, json\n",
    "import helpers\n",
    "progress=0\n",
    "with open(\"graphdata/lpg/summaries.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"embedding:vector\"])\n",
    "\n",
    "    jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "    for index, row in jsonObj.iterrows():\n",
    "    \n",
    "        if progress % 5 == 0:\n",
    "            print(str(progress))\n",
    "\n",
    "        # extract metadata about current press release\n",
    "        metadata=row['metadata']\n",
    "        m_keywords=metadata['keywords']\n",
    "        m_title=metadata['title']\n",
    "        m_doc=metadata['document_id']\n",
    "\n",
    "        summary=helpers.summarize(row['raw_text'])\n",
    "        summary_embedding=helpers.embedding_string(helpers.make_embedding(summary))\n",
    "    \n",
    "        with open(f\"summaries/{m_doc}.txt\", \"w\") as f:\n",
    "            f.write(summary)\n",
    "\n",
    "        writer.writerow([make_document_uri(m_doc), \"Document\", summary_embedding])\n",
    "        \n",
    "        progress+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9679b-18af-4845-ab2d-ad430304efbc",
   "metadata": {},
   "source": [
    "## Make chunks of docs\n",
    "Split press release documents into chunks, create embeddings for each chunk, and link the chunk to the document in the graph.\n",
    "\n",
    "Writes files to chunks folder.\n",
    "\n",
    "Creates the following CSV files:\n",
    "\n",
    "- chunks.csv - Document chunks and their embeddings.\n",
    "- chunk2doc.csv -Link chunks to docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00564f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_chunk_to_file(docfile, chunkidx, content):\n",
    "    file_name=f\"chunks/{docfile}_{str(chunkidx)}\"\n",
    "    text_file = open(file_name, \"w\")\n",
    "    text_file.write(content)\n",
    "    text_file.close()\n",
    "    return file_name\n",
    "\n",
    "all_splits=helpers.make_doc_splits(\"documents\")\n",
    "chunk2doc={}\n",
    "with open(\"graphdata/lpg/chunks.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"doc\", \"chunkidx:Integer\", \"embedding:vector\"])\n",
    "    \n",
    "    for index, s in enumerate(all_splits):\n",
    "        content=s.page_content\n",
    "        doc=s.metadata['source'].split(\"/\")[-1]\n",
    "        file_name = write_chunk_to_file(doc, index, content)\n",
    "        embedding=helpers.embedding_string(helpers.make_embedding(content))\n",
    "        chunk_local =f\"{doc}_{index}\" \n",
    "        chunk_uri = helpers.make_uri(f\"Chunk/{chunk_local}\")\n",
    "        doc_uri = make_document_uri(doc.split(\".\")[0])\n",
    "        writer.writerow([chunk_uri, \"Chunk\", doc, index, embedding])\n",
    "        chunk2doc[chunk_uri] = {\n",
    "            'chunk_uri': chunk_uri,\n",
    "            'doc_uri': doc_uri,\n",
    "            'chunk_local': chunk_local,\n",
    "            'doc_local': doc\n",
    "        }\n",
    "        \n",
    "# links chunks to docs\n",
    "with open(\"graphdata/lpg/chunk2doc.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\",\"~from\", \"~to\", \"~label\"])\n",
    "    \n",
    "    for cd in chunk2doc:\n",
    "        cdrec = chunk2doc[cd]\n",
    "        edge_id=helpers.make_uri(f\"chunkToDoc_{cdrec['chunk_local']}_{cdrec['doc_local']}\")\n",
    "        writer.writerow([edge_id, cdrec['chunk_uri'], cdrec['doc_uri'], \"belongsToDocument\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb4a60-0d11-4a74-ae1a-1cd7af8b0b62",
   "metadata": {},
   "source": [
    "## Make entity embeddings so I can search them better\n",
    "\n",
    "In scope:\n",
    "- taxonomical concept\n",
    "- orgs, persons, industries, services, products, locations\n",
    "- any loose structured objects \n",
    "- extracted entities\n",
    "- extracted events\n",
    "\n",
    "Not in scope:\n",
    "- relationships among structured entities\n",
    "- roles in extraction\n",
    "\n",
    "When a user queries, we extract entities from the QUESTION. We want reasonable searchability of those entities. The not-in-scope data is a small enumerated set. We can prompt the LLM to express predicates in the question from that set.\n",
    "\n",
    "In the code below, we populate the following:\n",
    "\n",
    "- entity_embeddings.csv - Embeddings of entities for searchability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016fc43-bf87-4eec-8676-3749670eaf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import helpers\n",
    "\n",
    "def make_entity_embedding(toks):\n",
    "    text=\" \".join(toks)\n",
    "    print(text)\n",
    "    return helpers.embedding_string(helpers.make_embedding(text))\n",
    "\n",
    "taxonomy_labels=['prefLabel', 'altLabels', 'broaders']\n",
    "struct_labels=['label', 'labels', 'ulabels', 'typeLabels', 'seeAlsoLabels', 'seeAlsoTypeLabels']\n",
    "extract_labels=['label', 'labels']\n",
    "entities_to_embed={\n",
    "    'orgs': struct_labels, \n",
    "    'persons': struct_labels, \n",
    "    'products': struct_labels,  \n",
    "    'industries': struct_labels, \n",
    "    'locations': struct_labels,  \n",
    "    'services': struct_labels, \n",
    "    'taxonomy_concepts': taxonomy_labels, \n",
    "    'extractions': extract_labels\n",
    "}\n",
    "\n",
    "with open(\"graphdata/lpg/entity_embeddings.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"embedding:vector\"])\n",
    "    \n",
    "    for ee in entities_to_embed:\n",
    "        print(ee)\n",
    "        with open(f'graphdata/lpg/{ee}.csv', newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',')\n",
    "            posof={}\n",
    "            for index, row in enumerate(csvreader):\n",
    "                if index==0:\n",
    "                    for rindex, r in enumerate(row):\n",
    "                        posof[r] = rindex\n",
    "                    print(posof)\n",
    "                else:\n",
    "                    _id = row[0]\n",
    "                    _label=row[1]\n",
    "                    candidates=[]\n",
    "                    for col in entities_to_embed[ee]:\n",
    "                        candidates += row[posof[col]].split(helpers.CELL_DELIM)\n",
    "                    embedding_text=make_entity_embedding(candidates)\n",
    "                    writer.writerow([_id, _label, embedding_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d705bf-484d-4acf-b2d8-e20bfd9933c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
