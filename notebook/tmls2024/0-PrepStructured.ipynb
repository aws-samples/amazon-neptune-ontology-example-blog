{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2d9b7-5ab8-4f26-b3b0-bd0c4786cf39",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "\n",
    "# Ask the Graph\n",
    "# Notebook 0: Prep Structured Data\n",
    "\n",
    "In this notebook we prepare structured organization data for the Neptune graph. We represent it in Labeled Property Graph (LPG) and Resource Description Framework (RDF) forms.\n",
    "\n",
    "You DO NOT need to run this notebook. The notebook produces output that is already prepared for you and available in a public bucket. But you may wish to review how we prepared that data. In that case, follow along with the logic below.\n",
    "\n",
    "Our input is sourced from s3://aws-neptune-customer-samples/tmls2024/source/. We download a local copy to the source folder:\n",
    "\n",
    "- dbpedia_orgsx.csv - from DBPedia query to obtain organizations\n",
    "- dbpedia_labelsx.csv - from DBPedia query to obtain labels for orgs and related entities\n",
    "- industry_taxonomy.json - A taxonomy for industry types\n",
    "\n",
    "We produce the following output locally:\n",
    "- graphdata/lpg - CSV files in Gremlin format to bulk-load to Neptune as LPG data\n",
    "- graphdata/rdf - Turtle files to bulk-load to Neptune as RDF data\n",
    "\n",
    "Specific files that we produce are the following. LPG files in graphdata/lpg are:\n",
    "\n",
    "- orgs.csv - Organizations\n",
    "- persons.csv - Persons belonging to orgs\n",
    "- industries.csv - Industries of organizations.\n",
    "- products.csv - Products from organizations.\n",
    "- services.csv - Services from organizations.\n",
    "- locations.csv - Locations of organizations.\n",
    "- rels.csv - Relationships of the above.\n",
    "- taxonomy_concepts.csv - A taxonomy of industry types, including alternate labels and broader concepts.\n",
    "- tax_concept_rels.csv - Link industries from industries.csv to taxonomy concepts for industry.\n",
    "\n",
    "RDF in graphdata/rdf are:\n",
    "- orgdata.ttl - Orgs, persons, industries, services, locations, rels\n",
    "- industry_taxonomy.ttl - A SKOS taxonomy of industry types, including alternate labels and broader concepts. Not produced here, but copied from source. We'll ingest this as is into Neptune later.\n",
    "- tax_concept_rels.ttl - Link industries from industries.csv to taxonomy concepts for industry.\n",
    "\n",
    "These files are also maintained in s3://aws-neptune-customer-samples/tmls2024/graphdata/\n",
    "\n",
    "TODO - data model, including what we've built so far\n",
    "\n",
    "In the next notebook, we continue the prep by creating embeddings, summaries, and extracted entities from a set of press releases that refer to the organizations from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dfeb6-ecfa-49cc-8811-12fc9f3b414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://aws-neptune-customer-samples-us-east-1/tmls2024/source/ source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4ba88-a461-45bf-a79b-4e19e91e8537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf graphdata summaries chunks documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mkdir -p graphdata graphdata/rdf graphdata/lpg summaries chunks documents\n",
    "cp source/industry_taxonomy.ttl graphdata/rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18867a0",
   "metadata": {},
   "source": [
    "## Get source data and create output folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ddf27-8724-428e-aff7-e7a472d9cdc7",
   "metadata": {},
   "source": [
    "## Create base org, person, product, and industry data\n",
    "\n",
    "We'll use DBPedia as a source.\n",
    "\n",
    "\n",
    "<details><summary>Click to view/hide how to get this data from DBPedia</summary>\n",
    "<p>\n",
    "\n",
    "To get a few of the orgs, run the following query on https://dbpedia.org/sparql against default named graph http://dbpedia.org. Set the result format to CSV. \n",
    "\n",
    "```\n",
    "select ?company ?otype ?olocation ?oparent ?pstatus ?pname ?pparent ?ptype ?purl ?fhomepage ?fname\n",
    "(GROUP_CONCAT(distinct ?oproduct;SEPARATOR=\"|\") AS ?oproducts)\n",
    "(GROUP_CONCAT(distinct ?oservice;SEPARATOR=\"|\") AS ?oservices)\n",
    "(GROUP_CONCAT(distinct ?osub;SEPARATOR=\"|\") AS ?osubs)\n",
    "(GROUP_CONCAT(distinct ?oindustry;SEPARATOR=\"|\") AS ?oindustries)\n",
    "(GROUP_CONCAT(distinct ?okeyPerson;SEPARATOR=\"|\") AS ?okeyPersons)\n",
    "(GROUP_CONCAT(distinct ?pindustry;SEPARATOR=\"|\") AS ?pindustries)\n",
    "where \n",
    "{\n",
    " values ?company { \n",
    "<http://dbpedia.org/resource/Amazon_(company)>\n",
    "<http://dbpedia.org/resource/Amazon_Web_Services>\n",
    "<http://dbpedia.org/resource/AbeBooks>\n",
    "\n",
    "<http://dbpedia.org/resource/Epic_Games>\n",
    "<http://dbpedia.org/resource/IMDb>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin>\n",
    "\n",
    "<http://dbpedia.org/resource/Ring_(company)>\n",
    "<http://dbpedia.org/resource/Rite_Aid>\n",
    "<http://dbpedia.org/resource/Rivian>\n",
    "\n",
    "<http://dbpedia.org/resource/Salesforce>\n",
    "<http://dbpedia.org/resource/Shutterfly>\n",
    "<http://dbpedia.org/resource/Spire_Global>\n",
    "<http://dbpedia.org/resource/Saint_Mary's_Hospital,_Manchester>\n",
    "<http://dbpedia.org/resource/Standard_Bank>\n",
    "\n",
    "<http://dbpedia.org/resource/The_Globe_and_Mail>\n",
    "<http://dbpedia.org/resource/Tsawwassen_First_Nation>\n",
    "<http://dbpedia.org/resource/Verizon_Communications>\n",
    "<http://dbpedia.org/resource/Visy>\n",
    "<http://dbpedia.org/resource/Whole_Foods_Market>\n",
    " } .\n",
    "OPTIONAL { ?company dbo:type ?otype . } .\n",
    "OPTIONAL { ?company dbo:location ?olocation . } .\n",
    "OPTIONAL { ?company dbo:parentCompany ?oparent . } .\n",
    "OPTIONAL { ?company dbo:product\t ?oproduct . } .\n",
    "OPTIONAL { ?company dbo:service\t ?oservice . } .\n",
    "OPTIONAL { ?company dbo:subsidiary\t ?osub . } .\n",
    "OPTIONAL { ?company dbo:industry ?oindustry . } .\n",
    "OPTIONAL { ?company dbo:keyPerson ?okeyPerson . } .\n",
    "\n",
    "OPTIONAL { ?company dbp:currentStatus ?pstatus . } .\n",
    "OPTIONAL { ?company dbp:industry ?pindustry . } .\n",
    "OPTIONAL { ?company dbp:name ?pname . } .\n",
    "OPTIONAL { ?company dbp:parent ?pparent . } .\n",
    "OPTIONAL { ?company dbp:type ?ptype . } .\n",
    "OPTIONAL { ?company dbp:url ?purl . } .\n",
    "\n",
    "OPTIONAL { ?company foaf:homepage ?fhomepage . } .\n",
    "OPTIONAL { ?company foaf:name ?fname . } .\n",
    "\n",
    "} \n",
    "GROUP BY ?company ?otype ?olocation ?oparent ?pstatus ?pname ?pparent ?ptype ?purl ?fhomepage ?fname\n",
    "ORDER BY ?company\n",
    "```\n",
    "\n",
    "To get labeling for URIs from results, \n",
    "\n",
    "```\n",
    "select distinct *\n",
    "where \n",
    "{\n",
    " values ?uri { \n",
    "<http://dbpedia.org/resource/The_Washington_Post>\n",
    " } .\n",
    "OPTIONAL { ?uri dbp:name|foaf:name|rdfs:label ?label . FILTER(lang(?label) = 'en') } .\n",
    "OPTIONAL { ?uri dbo:type ?dbotype . \n",
    "   OPTIONAL { ?dbotype dbp:name|foaf:name|rdfs:label ?dtlabel . FILTER(lang(?dtlabel) = 'en') } .\n",
    " } .\n",
    "OPTIONAL { ?uri rdfs:seeAlso ?seeAlso . \n",
    "  OPTIONAL { ?seeAlso dbo:type ?stype  .\n",
    "    OPTIONAL { ?stype dbp:name|foaf:name|rdfs:label ?stlabel . FILTER(lang(?stlabel) = 'en') } .\n",
    "  } .\n",
    "  OPTIONAL { ?seeAlso dbp:name|foaf:name|rdfs:label ?slabel . FILTER(lang(?slabel) = 'en') } .  \n",
    "} .\n",
    "\n",
    "} ```\n",
    "\n",
    "URIs to check include the following. DBPedia SPARQL engine throws a request size error if you use all URIs at once, so chunk them.\n",
    "```\n",
    "<http://dbpedia.org/resource/AbeBooks>\n",
    "<http://dbpedia.org/resource/Amazon_(company)>\n",
    "<http://dbpedia.org/resource/BookFinder.com>\n",
    "<http://dbpedia.org/resource/IberLibro>\n",
    "<http://dbpedia.org/resource/LibraryThing>\n",
    "<http://dbpedia.org/resource/A9.com>\n",
    "<http://dbpedia.org/resource/Alexa_Internet>\n",
    "<http://dbpedia.org/resource/Amazon.com>\n",
    "<http://dbpedia.org/resource/Amazon_Air>\n",
    "<http://dbpedia.org/resource/Amazon_Books>\n",
    "<http://dbpedia.org/resource/Amazon_Fresh>\n",
    "<http://dbpedia.org/resource/Amazon_Games>\n",
    "<http://dbpedia.org/resource/Amazon_Lab126>\n",
    "<http://dbpedia.org/resource/Amazon_Logistics>\n",
    "<http://dbpedia.org/resource/Amazon_Pharmacy>\n",
    "<http://dbpedia.org/resource/Amazon_Publishing>\n",
    "<http://dbpedia.org/resource/Amazon_Robotics>\n",
    "<http://dbpedia.org/resource/Amazon_Studios>\n",
    "<http://dbpedia.org/resource/Amazon_Web_Services>\n",
    "<http://dbpedia.org/resource/Audible_(service)>\n",
    "<http://dbpedia.org/resource/Blink_Home>\n",
    "<http://dbpedia.org/resource/Body_Labs>\n",
    "<http://dbpedia.org/resource/Book_Depository>\n",
    "<http://dbpedia.org/resource/ComiXology>\n",
    "<http://dbpedia.org/resource/Digital_Photography_Review>\n",
    "<http://dbpedia.org/resource/Goodreads>\n",
    "<http://dbpedia.org/resource/Graphiq>\n",
    "<http://dbpedia.org/resource/IMDb>\n",
    "<http://dbpedia.org/resource/MGM_Holdings>\n",
    "<http://dbpedia.org/resource/PillPack>\n",
    "<http://dbpedia.org/resource/Ring_Inc.>\n",
    "<http://dbpedia.org/resource/Souq.com>\n",
    "<http://dbpedia.org/resource/Twitch_Interactive>\n",
    "<http://dbpedia.org/resource/Whole_Foods_Market>\n",
    "<http://dbpedia.org/resource/Woot>\n",
    "<http://dbpedia.org/resource/Zappos>\n",
    "<http://dbpedia.org/resource/Zoox_Inc>\n",
    "<http://dbpedia.org/resource/Epic_Games>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin_Canada>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin_UK>\n",
    "<http://dbpedia.org/resource/Sikorsky_Aircraft>\n",
    "<http://dbpedia.org/resource/Ring_(company)>\n",
    "<http://dbpedia.org/resource/Rite_Aid>\n",
    "<http://dbpedia.org/resource/Bartell_Drugs>\n",
    "<http://dbpedia.org/resource/Rivian>\n",
    "<http://dbpedia.org/resource/Saint_Mary's_Hospital,_Manchester>\n",
    "<http://dbpedia.org/resource/Salesforce>\n",
    "<http://dbpedia.org/resource/Shutterfly>\n",
    "<http://dbpedia.org/resource/Spire_Global>\n",
    "<http://dbpedia.org/resource/Standard_Bank>\n",
    "<http://dbpedia.org/resource/The_Globe_and_Mail>\n",
    "<http://dbpedia.org/resource/Tsawwassen_First_Nation>\n",
    "<http://dbpedia.org/resource/Verizon_Communications>\n",
    "<http://dbpedia.org/resource/Visy>\n",
    "<http://dbpedia.org/resource/Book>\n",
    "<http://dbpedia.org/resource/Collectable>\n",
    "<http://dbpedia.org/resource/Ephemera>\n",
    "<http://dbpedia.org/resource/Fine_art>\n",
    "<http://dbpedia.org/resource/Out_of_print_books>\n",
    "<http://dbpedia.org/resource/Rare_book>\n",
    "<http://dbpedia.org/resource/Textbooks>\n",
    "<http://dbpedia.org/resource/Used_book>\n",
    "<http://dbpedia.org/resource/Amazon_Echo>\n",
    "<http://dbpedia.org/resource/Amazon_Fire_TV>\n",
    "<http://dbpedia.org/resource/Amazon_Fire_tablet>\n",
    "<http://dbpedia.org/resource/Amazon_Kindle>\n",
    "<http://dbpedia.org/resource/Fire_OS>\n",
    "<http://dbpedia.org/resource/Bink_Video>\n",
    "<http://dbpedia.org/resource/Epic_Games_Store>\n",
    "<http://dbpedia.org/resource/Fortnite>\n",
    "<http://dbpedia.org/resource/Gears_of_War>\n",
    "<http://dbpedia.org/resource/Unreal_(video_game_series)>\n",
    "<http://dbpedia.org/resource/Unreal_Engine>\n",
    "<http://dbpedia.org/resource/Atlas_V>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin_C-130J_Super_Hercules>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin_F-35_Lightning_II>\n",
    "<http://dbpedia.org/resource/Pharmacy>\n",
    "<http://dbpedia.org/resource/Car_battery>\n",
    "<http://dbpedia.org/resource/Electric_car>\n",
    "<http://dbpedia.org/resource/Bureau_de_change>\n",
    "<http://dbpedia.org/resource/Commercial_Banking>\n",
    "<http://dbpedia.org/resource/Insurance>\n",
    "<http://dbpedia.org/resource/Investment_Banking>\n",
    "<http://dbpedia.org/resource/Investment_Management>\n",
    "<http://dbpedia.org/resource/Private_Banking>\n",
    "<http://dbpedia.org/resource/Retail_Banking>\n",
    "<http://dbpedia.org/resource/Wealth_Management>\n",
    "<http://dbpedia.org/resource/Online_shopping>\n",
    "<http://dbpedia.org/resource/Amazon.ca>\n",
    "<http://dbpedia.org/resource/Amazon_Alexa>\n",
    "<http://dbpedia.org/resource/Amazon_Appstore>\n",
    "<http://dbpedia.org/resource/Amazon_Prime>\n",
    "<http://dbpedia.org/resource/Amazon_Prime_Video>\n",
    "<http://dbpedia.org/resource/Amazon_Web_Services>\n",
    "<http://dbpedia.org/resource/Ring_(company)>\n",
    "<http://dbpedia.org/resource/Twitch_(service)>\n",
    "<http://dbpedia.org/resource/Amazon_Luna>\n",
    "<http://dbpedia.org/resource/Amazon_Music>\n",
    "<http://dbpedia.org/resource/Amazon_Pay>\n",
    "<http://dbpedia.org/resource/Electric_vehicle_charging_network>\n",
    "<http://dbpedia.org/resource/Vehicle_insurance>\n",
    "<http://dbpedia.org/resource/Cloud_computing>\n",
    "<http://dbpedia.org/resource/Internet>\n",
    "<http://dbpedia.org/resource/E-commerce>\n",
    "<http://dbpedia.org/resource/Artificial_intelligence>\n",
    "<http://dbpedia.org/resource/Cloud_Computing>\n",
    "<http://dbpedia.org/resource/Consumer_electronics>\n",
    "<http://dbpedia.org/resource/Digital_distribution>\n",
    "<http://dbpedia.org/resource/Entertainment>\n",
    "<http://dbpedia.org/resource/Self-driving_cars>\n",
    "<http://dbpedia.org/resource/Supermarket>\n",
    "<http://dbpedia.org/resource/Grocery_store>\n",
    "<http://dbpedia.org/resource/Health_food_store>\n",
    "<http://dbpedia.org/resource/Video_game_industry>\n",
    "<http://dbpedia.org/resource/Retail>\n",
    "<http://dbpedia.org/resource/Automotive_industry>\n",
    "<http://dbpedia.org/resource/Energy_storage>\n",
    "<http://dbpedia.org/resource/Cloud_computing>\n",
    "<http://dbpedia.org/resource/Consulting>\n",
    "<http://dbpedia.org/resource/Enterprise_software>\n",
    "<http://dbpedia.org/resource/Aerospace>\n",
    "<http://dbpedia.org/resource/Banking>\n",
    "<http://dbpedia.org/resource/Packaging>\n",
    "<http://dbpedia.org/resource/Victoria,_British_Columbia>\n",
    "<http://dbpedia.org/resource/Bethesda,_Maryland>\n",
    "<http://dbpedia.org/resource/Santa_Monica,_California>\n",
    "<http://dbpedia.org/resource/Philadelphia,_Pennsylvania>\n",
    "<http://dbpedia.org/resource/United_States>\n",
    "<http://dbpedia.org/resource/Salesforce_Tower>\n",
    "<http://dbpedia.org/resource/San_Francisco,_California>\n",
    "<http://dbpedia.org/resource/Johannesburg>\n",
    "<http://dbpedia.org/resource/Standard_Bank_Centre>\n",
    "<http://dbpedia.org/resource/South_Africa>\n",
    "<http://dbpedia.org/resource/Chief_executive_officer>\n",
    "<http://dbpedia.org/resource/Andy_Jassy>\n",
    "<http://dbpedia.org/resource/Jeff_Bezos>\n",
    "<http://dbpedia.org/resource/President_(corporate_title)>\n",
    "<http://dbpedia.org/resource/Chairman>\n",
    "<http://dbpedia.org/resource/Chief_Creative_Officer>\n",
    "<http://dbpedia.org/resource/Chief_technical_officer>\n",
    "<http://dbpedia.org/resource/Mark_Rein_(software_executive)>\n",
    "<http://dbpedia.org/resource/James_D._Taiclet>\n",
    "<http://dbpedia.org/resource/CEO>\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030f5e3-86eb-43cb-8b56-2e2b4212025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, json\n",
    "import helpers\n",
    "\n",
    "# data structures for entities and relationships and common labeling\n",
    "orgs = {}\n",
    "persons={}\n",
    "locations={}\n",
    "products={}\n",
    "services={}\n",
    "industries={}\n",
    "labels={}\n",
    "encountered_types={}\n",
    "encountered_see_alsos={}\n",
    "main_entities=[orgs, persons, locations, products, services, industries]\n",
    "links=[]\n",
    "\n",
    "# convenience function to print URIs to get labels of\n",
    "# not used in main code, but to gather the labels CSV\n",
    "def get_entities_for_labeling():\n",
    "    def liri(s):\n",
    "        return f\"<{s}>\"\n",
    "        \n",
    "    for o in orgs:\n",
    "        print(liri(o))\n",
    "    for o in products:\n",
    "        print(liri(o))\n",
    "    for o in services:\n",
    "        print(liri(o))\n",
    "    for o in industries:\n",
    "        print(liri(o))\n",
    "    for o in locations:\n",
    "        print(liri(o))\n",
    "    for o in persons:\n",
    "        print(liri(o))\n",
    "\n",
    "# add record with distinct key to entity collection\n",
    "def add_record(coll, key):\n",
    "    rec = {}\n",
    "    if not (key in coll):\n",
    "        coll[key] = rec\n",
    "    else:\n",
    "        rec= coll[key]\n",
    "    return rec\n",
    "\n",
    "# If val is defined, add to record at key\n",
    "def add_single(dicto, key, val):\n",
    "    if str(val) == \"nan\" or str(val)==\"\":\n",
    "        return\n",
    "    if (not key in org):\n",
    "        dicto[key] = val\n",
    "\n",
    "# If val is defined, add to multi-val record at key\n",
    "def add_multi(dicto, key, val):\n",
    "    if str(val) == \"nan\" or str(val)==\"\":\n",
    "        return\n",
    "    if not key in dicto:\n",
    "        dicto[key] = [val]\n",
    "    elif not(val in dicto[key]):\n",
    "        dicto[key].append(val)\n",
    "\n",
    "# If val is defined, add each token of it to multi-val record at key\n",
    "def add_multi_arr(dicto, key, val):\n",
    "    if str(val) == \"nan\":\n",
    "        return\n",
    "    vals = val.split(\"|\")\n",
    "    for v in vals:\n",
    "        add_multi(dicto, key, v)\n",
    "        \n",
    "        \n",
    "# Add labeling for all entities in a collection\n",
    "def add_labels(coll):\n",
    "    for key in coll:\n",
    "        if key in labels:\n",
    "            # The URI in the collection has labeling, so let's add it\n",
    "            coll[key]['labels']=labels[key]\n",
    "            \n",
    "            # If it has a SeeAlso that is also a main entity type, link them\n",
    "            if 'seeAlsos' in labels[key]:\n",
    "                for sa in labels[key]['seeAlsos']:\n",
    "                    for m in main_entities:\n",
    "                        if sa in m:\n",
    "                            print(\"Gotcha\")\n",
    "                            add_link(key, sa, \"seeAlso\")\n",
    "                \n",
    "# add an edge (source, target, label). Will need this to map to LPG and RDF\n",
    "def add_link(source, target, label):\n",
    "    links.append([source, target, label])\n",
    "\n",
    "# Arrange labels and seeAlsos for this dataset that we sourced from DBPedia\n",
    "df = pd.read_csv(filepath_or_buffer=\"source/dbpedia_labelsx.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    uri = row['uri']\n",
    "    urirec = add_record(labels, uri)\n",
    "    \n",
    "    add_multi(urirec, \"labels\", row['label'])\n",
    "    add_multi(urirec, \"ulabels\", helpers.get_local_name(uri))\n",
    "    add_multi(urirec, \"types\",  row['dbotype'])\n",
    "    add_multi(urirec, \"typeLabels\",  row['dtlabel'])\n",
    "    add_multi(urirec, \"seeAlsos\", row['seeAlso'])\n",
    "    add_multi(urirec, \"seeAlsoTypes\",  row['stype'])\n",
    "    add_multi(urirec, \"seeAlsoTypeLabels\",  row['stlabel'])\n",
    "    add_multi(urirec, \"seeAlsoLabels\",  row['slabel'])\n",
    "    \n",
    "    urirec['label'] =  urirec['labels'][0] if 'labels' in urirec and len(urirec['labels']) > 0 else \"\"\n",
    "    \n",
    "    if not(str(row['dbotype']) == \"nan\" or str(row['dbotype'])==\"\"):\n",
    "        dbtype = row['dbotype']\n",
    "        dbtrec = add_record(encountered_types, dbtype)\n",
    "        add_multi(dbtrec, \"labels\", row['dtlabel'])\n",
    "        add_multi(dbtrec, \"ulabels\", helpers.get_local_name(dbtype))\n",
    "        dbtrec['label'] =  dbtrec['labels'][0] if 'labels' in dbtrec and len(dbtrec['labels']) > 0 else \"\"\n",
    "\n",
    "    if not(str(row['seeAlso']) == \"nan\" or str(row['seeAlso'])==\"\"):\n",
    "        sa = row['seeAlso']\n",
    "        sarec = add_record(encountered_see_alsos, sa)\n",
    "        add_multi(sarec, \"labels\", row['slabel'])\n",
    "        add_multi(sarec, \"ulabels\", helpers.get_local_name(sa))\n",
    "        add_multi(sarec, \"types\", row['stype'])\n",
    "        sarec['label'] =  sarec['labels'][0] if 'labels' in sarec and len(sarec['labels']) > 0 else \"\"\n",
    "\n",
    "    if not(str(row['stype']) == \"nan\" or str(row['stype'])==\"\"):\n",
    "        satype = row['stype']\n",
    "        satrec = add_record(encountered_types, satype)\n",
    "        add_multi(satrec, \"labels\", row['stlabel'])\n",
    "        add_multi(satrec, \"ulabels\", helpers.get_local_name(satype))\n",
    "        satrec['label'] =  satrec['labels'][0] if 'labels' in satrec and len(satrec['labels']) > 0 else \"\"\n",
    "\n",
    "# Build orgs dynamically; loop through results from DBPedia\n",
    "df = pd.read_csv(filepath_or_buffer=\"source/dbpedia_orgsx.csv\")\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # Company ID, which is an IRI, maps to org record in orgs dictionary\n",
    "    company_id = row['company']\n",
    "    org = add_record(orgs, company_id)\n",
    "    \n",
    "    add_single(org, 'leType', row['otype'])\n",
    "    add_single(org, 'leStatus', row['pstatus'])\n",
    "    add_single(org, 'parent', row['oparent']) \n",
    "    add_multi(org, 'locations', row['olocation'])\n",
    "    add_multi_arr(org, 'products', row['oproducts'])\n",
    "    add_multi_arr(org, 'services', row['oservices'])\n",
    "    add_multi_arr(org, 'subs', row['osubs'])    \n",
    "    add_multi_arr(org, 'industries', row['oindustries'])\n",
    "    add_multi_arr(org, 'keyPersons', row['okeyPersons'])\n",
    "\n",
    "    # Parent and subs are orgs too. Add them\n",
    "    if 'parent' in org:\n",
    "        add_record(orgs,org['parent'])\n",
    "        \n",
    "    if 'subs' in org:\n",
    "        for s in org['subs']:\n",
    "            add_record(orgs,s)\n",
    "\n",
    "# Add related entities\n",
    "for company_id in orgs:\n",
    "    org = orgs[company_id]\n",
    "    \n",
    "    if 'parent' in org:\n",
    "        add_link(company_id, org['parent'], 'hasParentOrg')\n",
    "        \n",
    "    if 'subs' in org:\n",
    "        for s in org['subs']:\n",
    "            add_link(s, company_id, 'subsidiaryOf')\n",
    "\n",
    "    if 'products' in org:\n",
    "        for p in org['products']:\n",
    "            add_record(products, p)\n",
    "            add_link(company_id, p, 'hasProduct')\n",
    "\n",
    "    if 'services' in org:\n",
    "        for s in org['services']:\n",
    "            add_record(services, s)\n",
    "            add_link(company_id, s, 'hasService')\n",
    "            \n",
    "    if 'locations' in org:\n",
    "        for l in org['locations']:\n",
    "            add_record(locations, l)\n",
    "            add_link(company_id, l, 'hasLocation')\n",
    "\n",
    "    if 'keyPersons' in org:\n",
    "        for k in org['keyPersons']:\n",
    "            add_record(persons, k)\n",
    "            add_link(company_id, k, 'hasKeyPerson')\n",
    "\n",
    "    if 'industries' in org:\n",
    "        for i in org['industries']:\n",
    "            add_record(industries, i)\n",
    "            add_link(company_id, i, 'hasIndustry')\n",
    "\n",
    "add_labels(orgs)\n",
    "add_labels(persons)\n",
    "add_labels(products)\n",
    "add_labels(services)\n",
    "add_labels(locations)\n",
    "add_labels(industries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b4fdc",
   "metadata": {},
   "source": [
    "## Map source data to LPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c540de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_node_file(filename, coll_label, coll, extra_prop_keys=[]):\n",
    "    headers = [\"~id\", \"~label\"]\n",
    "    labeling= [\"label\", \"labels\", \"ulabels\", \"types\", \"typeLabels\", \"seeAlsos\", \"seeAlsoTypes\", \"seeAlsoTypeLabels\", \"seeAlsoLabels\"]    \n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers + labeling + extra_prop_keys)\n",
    "        for p in coll:\n",
    "            # ID and node label\n",
    "            values = [p, coll_label]\n",
    "            for lk in labeling:\n",
    "                values.append(helpers.get_delim_string(coll[p]['labels'], lk))\n",
    "            for ex in extra_prop_keys:\n",
    "                values.append(helpers.get_delim_string(coll[p], ex))\n",
    "            writer.writerow(values)\n",
    "\n",
    "# write nodes to CSV\n",
    "write_node_file(\"graphdata/lpg/orgs.csv\", \"org\", orgs, [\"leType\", \"leStatus\"])\n",
    "write_node_file(\"graphdata/lpg/persons.csv\", \"person\", persons)\n",
    "write_node_file(\"graphdata/lpg/locations.csv\", \"location\", locations)\n",
    "write_node_file(\"graphdata/lpg/products.csv\", \"product\", products)\n",
    "write_node_file(\"graphdata/lpg/services.csv\", \"service\", services)\n",
    "write_node_file(\"graphdata/lpg/industries.csv\", \"industry\", industries)\n",
    "\n",
    "# write edges to CSV\n",
    "with open('graphdata/lpg/rels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\",\"~from\", \"~to\", \"~label\"])\n",
    "    for l in links:\n",
    "        source = l[0]\n",
    "        target = l[1]\n",
    "        label = l[2]\n",
    "        edge_id=f\"structuredLink_{source}_{target}\"\n",
    "        writer.writerow([ edge_id, source, target, label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6693ee",
   "metadata": {},
   "source": [
    "## Map source data to RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc17a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "from rdflib import Graph, Literal, RDF, RDFS, URIRef, XSD, OWL, BNode, DC\n",
    "\n",
    "rdf_file = helpers.rdf_open()\n",
    "\n",
    "def write_entities(coll_type, coll,  extra_prop_literal_keys=[], extra_prop_uri_keys=[]):\n",
    "    for p in coll:\n",
    "        helpers.rdf_write(rdf_file, URIRef(p), RDF.type, helpers.make_uri(\"OrgEntity\"))\n",
    "        helpers.rdf_write(rdf_file, URIRef(p), RDF.type, helpers.make_uri(coll_type))\n",
    "        for ex in extra_prop_literal_keys:\n",
    "            for val in helpers.get_delim_array(coll[p], ex):\n",
    "                helpers.rdf_write(rdf_file, URIRef(p), helpers.make_uri(ex), Literal(val))\n",
    "        for ex in extra_prop_uri_keys:\n",
    "            for val in helpers.get_delim_array(coll[p], ex):\n",
    "                helpers.rdf_write(rdf_file, URIRef(p), helpers.make_uri(ex), URIRef(val))\n",
    "        labels = coll[p]['labels']\n",
    "        if len(labels['label']) > 0:\n",
    "            helpers.rdf_write(rdf_file, URIRef(p), RDFS.label, Literal(labels['label']))\n",
    "        for ex in helpers.get_delim_array(labels, \"labels\"):\n",
    "            helpers.rdf_write(rdf_file, URIRef(p), helpers.make_uri(\"sourceLabel\"),Literal(ex))\n",
    "        for ex in helpers.get_delim_array(labels, \"ulabels\"):\n",
    "            helpers.rdf_write(rdf_file, URIRef(p), helpers.make_uri(\"sourceLabel\"),Literal(ex))\n",
    "        for ex in helpers.get_delim_array(labels, \"types\"):\n",
    "            helpers.rdf_write(rdf_file, URIRef(p), RDF.type, URIRef(ex))\n",
    "        for ex in helpers.get_delim_array(labels, \"seeAlsos\"):\n",
    "            helpers.rdf_write(rdf_file, URIRef(p), RDFS.seeAlso, URIRef(ex))\n",
    "            \n",
    "                \n",
    "# write nodes to CSV\n",
    "write_entities(\"Organization\", orgs, [\"leStatus\"], [\"leType\"])\n",
    "write_entities(\"Person\", persons)\n",
    "write_entities(\"Location\", locations)\n",
    "write_entities(\"Product\", products)\n",
    "write_entities(\"Service\", services)\n",
    "write_entities(\"Industry\", industries)\n",
    "\n",
    "# write the links\n",
    "for l in links:\n",
    "    source = l[0]\n",
    "    target = l[1]\n",
    "    label = l[2]\n",
    "    helpers.rdf_write(rdf_file, URIRef(source), helpers.make_uri(label), URIRef(target))\n",
    "\n",
    "# write encountered types\n",
    "for en in encountered_types:\n",
    "    if len(encountered_types[en]['label']) > 0:\n",
    "        helpers.rdf_write(rdf_file, URIRef(en), RDFS.label, Literal(encountered_types[en]['label']))\n",
    "\n",
    "for en in encountered_see_alsos:\n",
    "    if len(encountered_see_alsos[en]['label']) > 0:\n",
    "        helpers.rdf_write(rdf_file, URIRef(en), RDFS.label, Literal(encountered_see_alsos[en]['label']))\n",
    "                  \n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/orgdata.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48a8c1-1b56-4f68-a7c7-241b7c14ec43",
   "metadata": {},
   "source": [
    "## Add taxonomy\n",
    "\n",
    "We'll have some fun and have the LLM (Anthropic Sonnet 3.0) create a SKOS taxonomy for the world in industry!\n",
    "\n",
    "I tried this in the Bedrock playground. \n",
    "\n",
    "```\n",
    "Write a taxonomy for industry using SKOS in ntriples format. Include alternate labels. Make sure to include the following terms.\n",
    "\n",
    "Internet\n",
    "E-commerce\n",
    "Artificial intelligence\n",
    "Cloud Computing\n",
    "Consumer electronics\n",
    "Digital distribution\n",
    "Entertainment\n",
    "Self-driving cars\n",
    "Supermarket\n",
    "Grocery store\n",
    "Health food store\n",
    "Video game industry\n",
    "Retail\n",
    "Automotive industry\n",
    "Energy storage\n",
    "Cloud computing\n",
    "Consulting\n",
    "Enterprise software\n",
    "Aerospace\n",
    "Banking\n",
    "Packaging\n",
    "```\n",
    "\n",
    "Now let's add the taxonomy to both LPG and RDF models. \n",
    "\n",
    "The RDF file is already downloaded and is available in source/industry_taxonomy.ttl\n",
    "\n",
    "The LPG file is source/industry_taxonomy.json. We need to transform it from JSON to bulk-loadable CSV format.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69daf2b3-8da2-4b00-a8fd-276b96724097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read JSON file\n",
    "taxo_dict=None\n",
    "with open('source/industry_taxonomy.json') as taxo_file:\n",
    "    taxo_dict = json.load(taxo_file)\n",
    "\n",
    "# look at each entry, tie to the industry entity\n",
    "tax_labels={}\n",
    "concept_to_pref={}\n",
    "for entry in taxo_dict:\n",
    "    def get_skos_values(dicto, key, subkey='@value'):\n",
    "        vals=[]\n",
    "        if key in dicto:\n",
    "            for item in dicto[key]:\n",
    "                vals.append(item[subkey])\n",
    "        return vals\n",
    "                \n",
    "    concept=entry['@id']\n",
    "    prefs=get_skos_values(entry, 'http://www.w3.org/2004/02/skos/core#prefLabel')\n",
    "    if len(prefs)> 1:\n",
    "        raise Exception(\"Prefs too many labels \" + entry);\n",
    "    elif len(prefs) == 0:\n",
    "        continue\n",
    "    pref = prefs[0]\n",
    "    alts=get_skos_values(entry, 'http://www.w3.org/2004/02/skos/core#altLabel')\n",
    "    broaders=get_skos_values(entry, 'http://www.w3.org/2004/02/skos/core#broader', '@id')\n",
    "    entry_record=[concept, pref, alts, broaders]\n",
    "    tax_labels[pref.lower()] = entry_record\n",
    "    concept_to_pref[concept] = pref.lower()\n",
    "\n",
    "# prefer broaders as string, not URI\n",
    "for t in tax_labels:\n",
    "    broaders=[]\n",
    "    for b in tax_labels[t][3]:\n",
    "        broaders.append(concept_to_pref[b])\n",
    "    tax_labels[t].append(broaders)\n",
    "    \n",
    "# Now tie industries we already have on file to this taxonomy\n",
    "for i in industries:\n",
    "    if industries[i]['labels']['label'].lower() in tax_labels:\n",
    "        industries[i]['taxonomy']=tax_labels[industries[i]['labels']['label'].lower()]\n",
    "    else:\n",
    "        raise Exception(f\"Industry {i} with label {industries[i]['labels']['label']} has NO tax\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9f634-ec76-4802-a33b-c3e3f16ab21d",
   "metadata": {},
   "source": [
    "## Write taxonomy for LPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52137294-2b4d-4b70-8c65-8b5865d4fea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"graphdata/lpg/taxonomy_concepts.csv\", 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"prefLabel\", \"altLabels\", \"broaders\"])\n",
    "    for i in industries:\n",
    "        concept=industries[i]['taxonomy'][0]\n",
    "        pref=industries[i]['taxonomy'][1]\n",
    "        alts=industries[i]['taxonomy'][2]\n",
    "        broaders=industries[i]['taxonomy'][4]\n",
    "        \n",
    "        values=[concept, \"taxonomy_concept\", pref, helpers.CELL_DELIM.join(alts), helpers.CELL_DELIM.join(broaders)]\n",
    "        writer.writerow(values)\n",
    "\n",
    "# write edges to CSV\n",
    "with open('graphdata/lpg/tax_concept_rels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\",\"~from\", \"~to\", \"~label\"])\n",
    "    for i in industries:\n",
    "        concept=industries[i]['taxonomy'][0]\n",
    "        edge_id=f\"taxConceptLink_{i}_{concept}\"\n",
    "        writer.writerow([ edge_id, i, concept, \"hasTaxonomyConcept\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc294d-9cf8-4fba-820f-d29f5c04978a",
   "metadata": {},
   "source": [
    "## Write taxonomy for RDF\n",
    "This is mostly already done. Just a simple link suffices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe3266-a38e-43aa-8014-f59b48db72d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdf_file = helpers.rdf_open()\n",
    "\n",
    "for i in industries:\n",
    "    concept=industries[i]['taxonomy'][0]\n",
    "    helpers.rdf_write(rdf_file, URIRef(i), helpers.make_uri(\"hasConcept\"), URIRef(concept))\n",
    "\n",
    "helpers.rdf_close(rdf_file, \"graphdata/rdf/tax_concept_rels.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcbc92-a3fe-41a6-83cb-72c4905d112a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
