{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c381b59d",
   "metadata": {},
   "source": [
    "# Data Prep for Find and Link Demo\n",
    "\n",
    "Prepare publication and patient data for use in find/link entities demo accompanying blog post. \n",
    "\n",
    "This notebook is for demo purposes. You don't need to run this notebook, as we have already prepared the data for use in s3://aws-neptune-customer-samples/neptune-ent-res. If you're like most readers, you will want to focus on querying, rather than preparing, this data.\n",
    "\n",
    "If you wish to prepare the data yourself, copy this notebook to a notebook instance. It uses AWS CLI to download a few source files. To ensure you have all the right dependencies, run from a SageMaker notebook instance. \n",
    "\n",
    "The files it prepares are saved to disk on the instance, so ensure you have enough space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941badf0",
   "metadata": {},
   "source": [
    "## Library to create bulk-loadable files for labeled property graph (LPG) or resource description framework (RDF)\n",
    "\n",
    "See more about Neptune bulk loader here: https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Library to manage CSV and RDF files for bulk load\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import csv\n",
    "from rdflib import Graph, Literal, RDF, RDFS, URIRef, XSD, OWL, BNode\n",
    "\n",
    "NS = \"http://example.org/demo\" # default, but override as you see fit\n",
    "\n",
    "MAX_RDF_ROWS = 1000000\n",
    "\n",
    "def csv_open(fname, headers):\n",
    "    ff = []\n",
    "    ff.append(open(fname, \"w\"))\n",
    "    ff.append(csv.writer(ff[0]))\n",
    "    csv_write(ff, headers)\n",
    "    return ff\n",
    "\n",
    "def csv_close(ff):\n",
    "    ff[0].close()\n",
    "\n",
    "def csv_write(ff, arr):\n",
    "    ff[1].writerow(\"\" if pd.isna(a) or a=='-' else a for a in arr)\n",
    "    \n",
    "def rdf_open(fname):\n",
    "    toks = fname.rsplit(\".\")\n",
    "    ff = [Graph(), fname, toks[0], toks[1], 1]\n",
    "    return ff\n",
    "    \n",
    "def make_uri(name):\n",
    "    return URIRef(f\"{NS}/{name}\")\n",
    "\n",
    "def rdf_write(ff, s, p, o):\n",
    "    # rdblib does not flush, so break it up as it gets larger\n",
    "    if (ff[4] % MAX_RDF_ROWS) == 0:\n",
    "        old_count = ff[4]\n",
    "        new_fname = f\"{ff[2]}{old_count // MAX_RDF_ROWS}.{ff[3]}\"\n",
    "        print(\"RDF split \" + new_fname + \" on \" + str(old_count))\n",
    "        rdf_close(ff)\n",
    "        ff[0] = Graph()\n",
    "        ff[1]=new_fname\n",
    "        ff[4]=old_count\n",
    "\n",
    "    ff[4] = ff[4] + 1\n",
    "    ff[0].add((s, p, o))\n",
    "\n",
    "def rdf_close(ff):\n",
    "    ff[0].serialize(destination = ff[1], format='ntriples')\n",
    "    ff[0] = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c4d83",
   "metadata": {},
   "source": [
    "## Patient Data\n",
    "\n",
    "https://synthea.mitre.org/downloads\n",
    "\n",
    "Jason Walonoski, Mark Kramer, Joseph Nichols, Andre Quina, Chris Moesel, Dylan Hall, Carlton Duffett, Kudakwashe Dube, Thomas Gallagher, Scott McLachlan, Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record, Journal of the American Medical Informatics Association, Volume 25, Issue 3, March 2018, Pages 230â€“238, https://doi.org/10.1093/jamia/ocx079\n",
    "\n",
    "### First create folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e158e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf patients\n",
    "mkdir -p patients/source/downloads\n",
    "mkdir -p patients/source/generated\n",
    "mkdir -p patients/bulk/pg/nodes\n",
    "mkdir -p patients/bulk/pg/edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7df67c",
   "metadata": {},
   "source": [
    "### Download the synthea patient set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd patients/source/downloads\n",
    "wget https://mitre.box.com/shared/static/9iglv8kbs1pfi7z8phjl9sbpjk08spze.zip\n",
    "unzip 9iglv8kbs1pfi7z8phjl9sbpjk08spze.zip\n",
    "rm 9iglv8kbs1pfi7z8phjl9sbpjk08spze.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11b055",
   "metadata": {},
   "source": [
    "### Add a few duplicate patients. Will have similar name and location, but none of the encounters or payers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdca134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# This cell modifies source patients.csv as follows:\n",
    "# 1. It strips digits from the names. The source has names like Haley12. It strips to just Haley. \n",
    "#    This helps when doing full-text search of names.\n",
    "# 2. For a random patient it adds a new matching patient with same zip, SIMILAR first name and:\n",
    "# a) Same last name .\n",
    "# b) Similar last name.\n",
    "# 3. Randomly adds patients with same last name but different/dissimilar first name to the same zip as existing patient.\n",
    "\n",
    "def strip_digits(col, s):\n",
    "    if col in ['FIRST', 'LAST', 'MAIDEN']:\n",
    "        if s == 'nan':\n",
    "            return ''\n",
    "        return re.sub(r'\\d+', '', s)\n",
    "    else:\n",
    "        return s\n",
    "        \n",
    "def has_val(cell):\n",
    "    return not(cell != cell) and len(str(cell)) > 0 and str(cell) != 'nan'\n",
    "        \n",
    "patients_df = pd.read_csv('patients/source/downloads/10k_synthea_covid19_csv/patients.csv')\n",
    "new_rows = []\n",
    "cols = patients_df.columns\n",
    "zips_ldiff = []\n",
    "for index, row in patients_df.iterrows():\n",
    "    id = row['Id'] \n",
    "    this_row = {}\n",
    "    for c in cols:\n",
    "        this_row[str(c)] = strip_digits(c, str(row[c]))\n",
    "    new_rows.append(this_row)\n",
    "    \n",
    "    # random row\n",
    "    if has_val(this_row['ZIP']) and random.randint(1, 30) < 7:\n",
    "        mod_row = this_row.copy()\n",
    "        \n",
    "        #new ID\n",
    "        mod_row['Id'] = \"x\" + id\n",
    "        \n",
    "        # munge the first name\n",
    "        fn = this_row['FIRST']\n",
    "        mod_row['FIRST'] = fn[:1] + 'x' + fn[1+1:]\n",
    "        \n",
    "        # munge the last name\n",
    "        lmod = False\n",
    "        if random.randint(1, 20) < 7:\n",
    "            lmod = True\n",
    "            ln = this_row['LAST']\n",
    "            mod_row['LAST'] = ln[:1] + 'z' + ln[1+1:]\n",
    "\n",
    "\n",
    "        # debugging\n",
    "        compare_string = \"*\".join([this_row['Id'], this_row['FIRST'], this_row['LAST'], this_row['ZIP'], \n",
    "            mod_row['FIRST'], mod_row['LAST']])\n",
    "        \n",
    "        if lmod:\n",
    "            zips_ldiff.append(this_row['ZIP'])\n",
    "            print(\"Patient diff last, same zip \" +  compare_string)\n",
    "        else:\n",
    "            print(\"Patient same last, same zip \" +  compare_string)\n",
    "\n",
    "        new_rows.append(mod_row)\n",
    "        \n",
    "        # add some more folks with same last name\n",
    "        MATE_NAMES = ['Bzob', 'Mzike', 'Jzoe', 'Jzames', 'Czlyde', 'Azthony', 'Fzord']\n",
    "        num_mates = random.randint(2, 5)\n",
    "        for n in range(0, num_mates):\n",
    "            mate_row = mod_row.copy()\n",
    "            mate_row['Id'] = \"y\" + str(n) + id\n",
    "            mate_row['FIRST'] = MATE_NAMES[n]\n",
    "            mate_row['SSN'] = ''\n",
    "            mate_row['BIRTHDATE'] = ''\n",
    "            mate_row['DEATHDATE'] = ''\n",
    "            mate_row['MAIDEN'] = ''\n",
    "            new_rows.append(mate_row)\n",
    "    \n",
    "new_patients_df = pd.DataFrame(new_rows)\n",
    "new_patients_df.to_csv('patients/source/generated/newpatients.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5f6e8",
   "metadata": {},
   "source": [
    "### Will use patient set expanded with duplicates in place of the one downloaded from synthea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2124b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mv patients/source/generated/newpatients.csv  patients/source/downloads/10k_synthea_covid19_csv/patients.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e306e",
   "metadata": {},
   "source": [
    "### Convert synthea data for Neptune bulk load formats: both LPG and RDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ffb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS = \"http://example.org/patientgraph\"\n",
    "LINKS = [\"ORGANIZATION\", \"PATIENT\", \"PROVIDER\", \"PAYER\", \"ENCOUNTER\"]\n",
    "\n",
    "edge_file = csv_open(\"patients/bulk/pg/edges/edges.csv\", [\"~from\",\"~to\",\"~id\",\"~label\"])\n",
    "\n",
    "def set_props(df, nep_headers, skip_links):\n",
    "    l = len(df.columns)\n",
    "    for i in range(l):\n",
    "        col_name = df.columns[i]\n",
    "        col_type = df.dtypes[i]\n",
    "        if not(col_name in LINKS) or skip_links == False: \n",
    "            if col_type == 'float64':\n",
    "                nep_headers.append(f\"{col_name}:double\")\n",
    "            elif col_type == 'int64':\n",
    "                nep_headers.append(f\"{col_name}:long\")\n",
    "            else:\n",
    "                nep_headers.append(f\"{col_name}:string\")\n",
    "\n",
    "def make_persource_node_headers(df):\n",
    "    nep_headers = ['~id', '~label']\n",
    "    set_props(df, nep_headers, False)\n",
    "    return nep_headers\n",
    "\n",
    "def make_persource_edge_headers(df):\n",
    "    nep_headers = [\"~from\",\"~to\",\"~id\"]\n",
    "    set_props(df, nep_headers, True)\n",
    "    nep_headers.append(\"~label\")\n",
    "    return nep_headers\n",
    "\n",
    "def make_loader_file(source_csv, label):\n",
    "    print(\"load \" + label)\n",
    "    df = pd.read_csv(source_csv)\n",
    "    if 'ZIP' in df.columns:\n",
    "        convert_dict = {'ZIP': object} \n",
    "        df = df.astype(convert_dict)\n",
    "    node_headers = make_persource_node_headers(df)\n",
    "    node_file = csv_open(f\"patients/bulk/pg/nodes/{label}.csv\", node_headers)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        nodeid = row['Id'] if 'Id' in row else str(index)\n",
    "        uri = make_uri(f\"{label.lower()}/{nodeid}\")\n",
    "        myrow = [str(uri), label]\n",
    "        for c in df.columns:\n",
    "            myrow.append(row[c])\n",
    "            if c in LINKS:\n",
    "            \n",
    "                # special handling - encounter/patient reverse direction\n",
    "                linkuri = make_uri(f\"{c.lower()}/{row[c]}\")\n",
    "                source_node = uri\n",
    "                target_node = linkuri\n",
    "                link_label = c\n",
    "                if label=='Encounter' and c=='PATIENT':\n",
    "                    source_node = linkuri\n",
    "                    target_node = uri\n",
    "                    link_label = \"ENCOUNTER\"\n",
    "                edgeuri = make_uri(f\"edge-{source_node}-{target_node}\")\n",
    "                csv_write(edge_file, [str(source_node), str(target_node), edgeuri, f\"has{link_label}\"])\n",
    "            \n",
    "        csv_write(node_file, myrow)\n",
    "    csv_close(node_file)\n",
    "\n",
    "def make_loader_file_pt(source_csv):\n",
    "    print(\"load pt\")\n",
    "    df = pd.read_csv(source_csv)\n",
    "    if 'ZIP' in df.columns:\n",
    "        convert_dict = {'ZIP': object} \n",
    "        df = df.astype(convert_dict)\n",
    "    edge_headers = make_persource_edge_headers(df)\n",
    "    ptedge_file = csv_open(f\"patients/bulk/pg/edges/patient_payer_edges.csv\", edge_headers)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        patient_uri = make_uri(f\"patient/{row['PATIENT']}\")\n",
    "        payer_uri = make_uri(f\"payer/{row['PAYER']}\")\n",
    "        edge_uri = make_uri(f\"edge-{patient_uri}-{payer_uri}-{str(index)}\")\n",
    "        myrow = [str(patient_uri), str(payer_uri), str(edge_uri)]\n",
    "        for c in df.columns:\n",
    "            if not(c in LINKS):\n",
    "                myrow.append(row[c])\n",
    "        myrow.append(\"hasPAYER\")\n",
    "        csv_write(ptedge_file, myrow)\n",
    "    csv_close(ptedge_file)\n",
    "\n",
    "make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/patients.csv\", 'Patient')\n",
    "make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/organizations.csv\", 'Organization')\n",
    "make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/providers.csv\", 'Provider')\n",
    "make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/payers.csv\", 'Payer')\n",
    "make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/encounters.csv\", 'Encounter')\n",
    "make_loader_file_pt(\"patients/source/downloads/10k_synthea_covid19_csv/payer_transitions.csv\")\n",
    "\n",
    "# The remainder are interesting to link from encounter. To save space, we'll leave them. \n",
    "#make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/allergies.csv\", 'Allergy')\n",
    "#make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/conditions.csv\", 'Condition')\n",
    "#make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/medications.csv\", 'Medication')\n",
    "#make_loader_file(\"patients/source/downloads/10k_synthea_covid19_csv/careplans.csv\", 'Careplan')\n",
    "\n",
    "csv_close(edge_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097aa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm patients.zip\n",
    "zip patients.zip -r patients/bulk/pg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7c282",
   "metadata": {},
   "source": [
    "## Publications Data\n",
    "DBLP/ACM dataset, known to have lots of matches.\n",
    "\n",
    "Transform the Neptune CSV:\n",
    "- Authors\n",
    "- Publications - with embeddings of title added\n",
    "- Edges: hasAuthor from Publication to Author\n",
    "\n",
    "\n",
    "### Setup text-to-vector mapping\n",
    "For use in vector similarity example\n",
    "Will use all-MiniLM-L6-v2 from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embeddings(sentences):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "    \n",
    "def get_str_embedding(embedding):\n",
    "    return ';'.join([str(x) for x in embedding])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ff0df",
   "metadata": {},
   "source": [
    "### Create folder structure for publications data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf publications\n",
    "mkdir -p publications/source/downloads\n",
    "mkdir -p publications/dedup/prep/pg/nodes\n",
    "mkdir -p publications/dedup/prep/pg/edges\n",
    "mkdir -p publications/dedup/prep/rdf\n",
    "mkdir -p publications/dedup/bulk/pg/nodes\n",
    "mkdir -p publications/dedup/bulk/pg/edges\n",
    "mkdir -p publications/dedup/bulk/rdf\n",
    "mkdir -p publications/vss/bulk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8563ed6",
   "metadata": {},
   "source": [
    "### Download publication source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd publications/source/downloads\n",
    "aws s3 cp s3://ml-transforms-public-datasets-us-east-1/dblp-acm/records/dblp_acm_records.csv .\n",
    "\n",
    "cd ..\n",
    "aws s3 cp s3://aws-neptune-customer-samples/neptune-ent-res/publications/dedup/source/dblp_acm_matches_ordered.csv dblp_acm_matches_ordered.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688952d0",
   "metadata": {},
   "source": [
    "### Add a few interesting fakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# ADD A FEW INTERESTING FAKES\n",
    "\n",
    "cd publications/source/downloads\n",
    "echo journals/haveyweb/1,Design and findings and next steps for Java implementation of Telegraph Dataflow,Michael Havey,HaveyWeb,2023,DBLP >> dblp_acm_records.csv\n",
    "echo journals/haveyweb/2,Tetanus cases increase Emerg units in September,Michael Havey,HaveyWeb,2023,DBLP >> dblp_acm_records.csv\n",
    "echo journals/haveyweb/3,ER staff reports lockjaw diagnoses rose 5 percent after Labor Day,Michael Havey,HaveyWeb,2023,DBLP >> dblp_acm_records.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de90a04",
   "metadata": {},
   "source": [
    "### Take initial pass through publications. No embeddings or matches yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82389c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create property graph and RDF representations of publications list. \n",
    "In this cell, start by separating into 3 CSVs plus one ntriples\n",
    "'''\n",
    "\n",
    "NS = \"http://example.org/pubgraph\"\n",
    "\n",
    "pub_file = csv_open(\"publications/dedup/prep/pg/nodes/pub.csv\", [\"~id\",\"~label\",\n",
    "    \"sourceid:string\", \"source:string\", \"venue:string\", \"year:integer\", \"title:string\"])\n",
    "aut_file = csv_open(\"publications/dedup/bulk/pg/nodes/author.csv\", [\"~id\",\"~label\",\"name:string\"])\n",
    "pa_edge_file = csv_open(\"publications/dedup/bulk/pg/edges/paedge.csv\", [\"~from\",\"~to\",\"~id\",\"~label\"])\n",
    "rdf_file = rdf_open(\"publications/dedup/bulk/rdf/pubs.nt\")\n",
    "\n",
    "distinct_authors={}\n",
    "df = pd.read_csv('publications/source/downloads/dblp_acm_records.csv')\n",
    "for index, row in df.iterrows():\n",
    "    source_id = row['id']\n",
    "    source_id_esc = urllib.parse.quote(source_id)\n",
    "    venue = row['venue']\n",
    "    year = row['year']\n",
    "    source = row['source']\n",
    "    title = row['title']\n",
    "    pub_uri = make_uri(f\"{source}/{source_id_esc}\")\n",
    "    csv_write(pub_file, [str(pub_uri), \"Publication\" ,source_id, source, venue, year, title])\n",
    "    rdf_write(rdf_file, pub_uri, RDF.type, make_uri(\"Publication\"))\n",
    "    rdf_write(rdf_file, pub_uri, make_uri(\"hasSource\"), make_uri(source))\n",
    "    rdf_write(rdf_file, pub_uri, make_uri(\"sourceID\"), Literal(source_id))\n",
    "    rdf_write(rdf_file, pub_uri, make_uri(\"venue\"), Literal(venue))\n",
    "    rdf_write(rdf_file, pub_uri, make_uri(\"year\"), Literal(year))\n",
    "    rdf_write(rdf_file, pub_uri, make_uri(\"title\"), Literal(title))\n",
    "\n",
    "    if not pd.isna(row['authors']):\n",
    "        auts = row['authors'].split(\",\")\n",
    "        for a in auts:\n",
    "            sa = a.strip()\n",
    "            sau = urllib.parse.quote(sa)\n",
    "            author_uri=make_uri(f\"author/{sau}\")\n",
    "            edge_uri =  make_uri(f\"hasAuthor/{source}/{source_id}/{author_uri}\")\n",
    "            csv_write(pa_edge_file, [str(pub_uri), str(author_uri), str(edge_uri), \"hasAuthor\"])\n",
    "            rdf_write(rdf_file, pub_uri, make_uri(\"hasAuthor\"), author_uri)\n",
    "            if not(sa in distinct_authors):\n",
    "                csv_write(aut_file, [str(author_uri), \"Author\", sa])\n",
    "                rdf_write(rdf_file, author_uri, RDF.type, make_uri(\"Author\"))\n",
    "                rdf_write(rdf_file, author_uri, make_uri(\"name\"), Literal(sa))\n",
    "                distinct_authors[sa] = \"a\"\n",
    "\n",
    "csv_close(pub_file)\n",
    "csv_close(aut_file)\n",
    "csv_close(pa_edge_file)\n",
    "rdf_close(rdf_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59807275",
   "metadata": {},
   "source": [
    "### Add embeddings to publication. This is for VSS demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9dfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For VSS demo, write publications with embeddings.\n",
    "'''\n",
    "\n",
    "pub_file = csv_open(\"publications/vss/bulk/pub.csv\", [\"~id\",\"~label\",\n",
    "    \"sourceid:string\", \"source:string\", \"venue:string\", \"year:integer\", \"title:string\", \"embedding:vector\"])\n",
    "\n",
    "iter = 1\n",
    "df = pd.read_csv('publications/dedup/prep/pg/nodes/pub.csv')\n",
    "for index, row in df.iterrows():\n",
    "    iter += 1\n",
    "    if iter % 100 == 0:\n",
    "        print(str(iter))\n",
    "    myrow = [\n",
    "        row[\"~id\"],\n",
    "        row[\"~label\"], \n",
    "        row[\"sourceid:string\"], \n",
    "        row[\"source:string\"], \n",
    "        row[\"venue:string\"], \n",
    "        row[\"year:integer\"], \n",
    "        row[\"title:string\"], \n",
    "        get_str_embedding(get_embeddings(row[\"title:string\"]))\n",
    "    ]\n",
    "    csv_write(pub_file, myrow)\n",
    "\n",
    "csv_close(pub_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a77a2",
   "metadata": {},
   "source": [
    "### Build dedup match representation. This is for the dedup demo only. This demo is not discussed in the blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c330767",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For dedup exploration:\n",
    "- Two new labels for a publication: AuthoritativePublication, LinkedPublication\n",
    "- New relationship: matches\n",
    "\n",
    "We don't need embeddings here.\n",
    "\n",
    "As input, dblp_acm_matches_ordered.csv is sorted by match id. Within a match group most recent DBLP comes first\n",
    "and is considered authoritative\n",
    "\n",
    "'''\n",
    "\n",
    "pub_file = csv_open(\"publications/dedup/bulk/pg/nodes/pub.csv\", [\"~id\",\"~label\",\n",
    "    \"sourceid:string\", \"source:string\", \"venue:string\", \"year:integer\", \"title:string\"])\n",
    "pp_edge_file = csv_open(\"publications/dedup/bulk/pg/edges/ppedge.csv\", [\"~from\",\"~to\",\"~id\", \n",
    "    \"matchSource:string\", \"matchAlg:string\", \"matchReason:string\", \"~label\"])\n",
    "rdf_file = rdf_open(\"publications/dedup/bulk/rdf/matches.nt\")\n",
    "\n",
    "\n",
    "'''\n",
    "Build dictionary from match file\n",
    "'''\n",
    "pub_match_dict = {}\n",
    "match_df = pd.read_csv('publications/source/dblp_acm_matches_ordered.csv')\n",
    "last_match_id = \"\"\n",
    "last_lead_id = \"\"\n",
    "last_lead_source = \"\"\n",
    "for index, row in match_df.iterrows():\n",
    "    source_id = row['id']\n",
    "    match_id = row['match_id']\n",
    "    source=row['source']\n",
    "    if match_id == last_match_id:\n",
    "        lead_id_esc = urllib.parse.quote(last_lead_id)\n",
    "        lead_uri = make_uri(f\"{last_lead_source}/{lead_id_esc}\")\n",
    "        pub_match_dict[source_id] = {\n",
    "            'alt_label': 'LinkedPublication', \n",
    "            'group': match_id,\n",
    "            'lead': lead_uri\n",
    "        }\n",
    "    else:\n",
    "        last_lead_id = source_id\n",
    "        last_match_id = match_id\n",
    "        last_lead_source = source\n",
    "        pub_match_dict[source_id] = {\n",
    "            'alt_label': 'AuthoritativePublication'\n",
    "        }\n",
    "\n",
    "def get_match_detail(source_id):\n",
    "    if source_id in pub_match_dict:\n",
    "        return pub_match_dict[source_id]\n",
    "    else:\n",
    "        return {\n",
    "            'alt_label': 'AuthoritativePublication'\n",
    "        }\n",
    "        \n",
    "\n",
    "'''\n",
    "Rebuild publications based on matchs\n",
    "'''\n",
    "\n",
    "pub_df = pd.read_csv('publications/dedup/prep/pg/nodes/pub.csv')\n",
    "for index, row in pub_df.iterrows():\n",
    "    myrow = [\n",
    "        row[\"~id\"],\n",
    "        row[\"~label\"], \n",
    "        row[\"sourceid:string\"], \n",
    "        row[\"source:string\"], \n",
    "        row[\"venue:string\"], \n",
    "        row[\"year:integer\"], \n",
    "        row[\"title:string\"]\n",
    "    ]\n",
    "    match_detail = get_match_detail(myrow[2])\n",
    "    alt_label = match_detail['alt_label']\n",
    "    myrow[1] = f\"{myrow[1]};{alt_label}\"\n",
    "    csv_write(pub_file, myrow)\n",
    "    pub_uri = URIRef(myrow[0])\n",
    "    rdf_write(rdf_file, pub_uri, RDF.type, make_uri(alt_label)) \n",
    "    \n",
    "    if 'lead' in match_detail:\n",
    "        match_source=\"dedup\"\n",
    "        match_alg=\"FindMatches\"\n",
    "        match_reason=match_detail['group']\n",
    "        lead_uri = match_detail['lead']\n",
    "        edge_uri = make_uri(f\"matches-{str(pub_uri)}-{str(lead_uri)}\")\n",
    "        csv_write(pp_edge_file, [lead_uri, myrow[0], edge_uri, match_source, match_alg, match_reason, \"matches\"])\n",
    "\n",
    "        # Avoid the bnode here to make ingest repeatable\n",
    "        match_uri = make_uri(f\"match-{row['sourceid:string']}\")\n",
    "        rdf_write(rdf_file, pub_uri, make_uri(\"hasMatch\"), match_uri)\n",
    "        rdf_write(rdf_file, match_uri, make_uri(\"hasLead\"), lead_uri)\n",
    "        rdf_write(rdf_file, match_uri, make_uri(\"hasMatchSource\"), make_uri(match_source))\n",
    "        rdf_write(rdf_file, match_uri, make_uri(\"hasMatchAlgorithm\"), make_uri(match_alg))\n",
    "        rdf_write(rdf_file, match_uri, make_uri(\"matchReason\"), Literal(match_reason))\n",
    "        \n",
    "csv_close(pub_file)\n",
    "csv_close(pp_edge_file)\n",
    "rdf_close(rdf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
