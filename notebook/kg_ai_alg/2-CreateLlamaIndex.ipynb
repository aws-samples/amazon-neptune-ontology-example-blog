{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2d9b7-5ab8-4f26-b3b0-bd0c4786cf39",
   "metadata": {},
   "source": [
    "# KGC 2024 MasterClass: Generating and analyzing knowledge graphs using GenAI and Neptune Analytics\n",
    "# Notebook 2: Setup LlamaIndex \n",
    "\n",
    "This notebook creates a LlamaIndex graph store and vector store of press release data in the Neptune Analytics graph. It coexists with the organizational knowledge graph and related Comprehend extraction results. \n",
    "\n",
    "Here is our data model.\n",
    "\n",
    "<img src=\"images/kgc_model.png\">\n",
    "\n",
    "LlamaIndex objects are colored pink. They are mostly independent of the data we loaded in the previous notebook. Their only link is that the DOCUMENT node (white box) representing a press release is linked to Chunk nodes created by the LlamaIndex vector store.\n",
    "\n",
    "The next figure depicts our design.\n",
    "\n",
    "<img src=\"images/kgc_design.png\">\n",
    "\n",
    "The LlamaIndex portion is shown in the upper third of the figure. \n",
    "\n",
    "To run this notebook you need a Neptune Analytics graph that is accessible from this notebook instance. You also need an S3 bucket in the same region. We will stage chunk-to-document links in that bucket to batch-load to Neptune Analytics graph. See README.md for detailed setup instructions.\n",
    "\n",
    "## Install LlamaIndex libraries\n",
    "We use Neptune graph and vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d76f6c-3767-49d3-acc9-837356829281",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install llama-index llama-index-vector-stores-neptune llama-index-graph-stores-neptune  llama-index-llms-bedrock llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe84ad",
   "metadata": {},
   "source": [
    "## Build LlamaIndex vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_notebook as gn\n",
    "config = gn.configuration.get_config.get_config()\n",
    "\n",
    "region = config.aws_region\n",
    "graph_identifier=config._host.split(\".\")[0]\n",
    "s3_bucket = f\"s3://aws-neptune-customer-samples-{region}/kgc2024_na/rawtext/\"\n",
    "\n",
    "graph_identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$s3_bucket\"\n",
    "\n",
    "aws s3 sync $1 rawtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7d9e7-9067-4587-982b-44520cb76ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, Settings\n",
    "from llama_index.vector_stores.neptune import NeptuneAnalyticsVectorStore\n",
    "from llama_index.core import download_loader, SimpleDirectoryReader\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# define LLM\n",
    "llm = Bedrock(model=\"anthropic.claude-v2\")\n",
    "embed_model = BedrockEmbedding(model=\"amazon.titan-embed-text-v1\")\n",
    "\n",
    "# Set global LLM settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Define Vector Store\n",
    "vector_store = NeptuneAnalyticsVectorStore(graph_identifier=graph_identifier, embedding_dimension=1536)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"rawtext\").load_data()\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    "    include_embeddings=True,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439945b6-f73b-4dfa-8b7e-98e4dd1f5fe2",
   "metadata": {},
   "source": [
    "## Build LlamaIndex Graph store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf8ce3-0c4d-499e-b40a-65c6003094da",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.core import StorageContext, KnowledgeGraphIndex, Settings\n",
    "from llama_index.graph_stores.neptune import NeptuneAnalyticsGraphStore\n",
    "from llama_index.core import download_loader, SimpleDirectoryReader\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# define LLM\n",
    "llm = Bedrock(model=\"anthropic.claude-v2\")\n",
    "embed_model = BedrockEmbedding(model=\"amazon.titan-embed-text-v1\")\n",
    "\n",
    "# Set global LLM settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Define Graph Store\n",
    "graph_store = NeptuneAnalyticsGraphStore(graph_identifier=graph_identifier)\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"rawtext\").load_data()\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "     documents=documents,\n",
    "     storage_context=storage_context,\n",
    "     max_triplets_per_chunk=50,\n",
    "     include_embeddings=True,\n",
    "     show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f4ad4",
   "metadata": {},
   "source": [
    "## Explore these stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f871b44",
   "metadata": {},
   "source": [
    "### Ask questions of the stores\n",
    "Start with the graph store, then same query to the vector store.\n",
    "\n",
    "You can also try the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf76f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "query_engine = kg_index.as_query_engine()\n",
    "response = query_engine.query(\"Tell me about mergers involving Amazon\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"Tell me about mergers involving Amazon\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a182e",
   "metadata": {},
   "source": [
    "### Get stats to see node and edge types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9f1ad",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%summary pg --detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60905b77",
   "metadata": {},
   "source": [
    "### Explore the graph store\n",
    "Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ce7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc\n",
    "\n",
    "MATCH (s:Entity)-[p]->(o)\n",
    "RETURN s.id, type(p), o.id\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4bd21",
   "metadata": {},
   "source": [
    "### Look at the chunks in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc\n",
    "\n",
    "MATCH (n:Chunk) \n",
    "CALL neptune.algo.vectors.get(n)\n",
    "YIELD embedding\n",
    "RETURN n.file_name, id(n), n.text, embedding\n",
    "LIMIT 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889870e",
   "metadata": {},
   "source": [
    "### Do vector similarity search on vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4af5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embed_model.get_text_embedding(\"kindle\")\n",
    "embparams={'emb': embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc -qp embparams\n",
    "\n",
    "WITH $emb as emb\n",
    "CALL neptune.algo.vectors.topKByEmbedding(emb)\n",
    "YIELD embedding, node, score\n",
    "RETURN id(node), node.file_name, node.text\n",
    "LIMIT 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c83f1",
   "metadata": {},
   "source": [
    "### Find chunks similar to a specific chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc \n",
    "\n",
    "MATCH(n:Chunk {`~id`: \"50e02811-229c-47c1-a241-7317183ab6d1\"})\n",
    "CALL neptune.algo.vectors.topKByNode(n)\n",
    "YIELD node, score\n",
    "WHERE n.file_name <> node.file_name\n",
    "RETURN score, id(n) as sourceNodeId, n.file_name as sourceNodeFile, \n",
    "id(node) as matchedNodeId, node.file_name as matchedNodeFile, \n",
    "n.text as sourceText, node.text as matchedText\n",
    "ORDER BY score desc\n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095c354-6ad2-4ce3-aff0-c4dbdee32e4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Link Comprehend documents to chunks created by LlamaIndex vector store\n",
    "### Let's summarize the chunks and how they link to doc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cb5b9-e23d-4070-8600-040eaf0d5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc --store-to chunks_per_file\n",
    "\n",
    "MATCH(c:Chunk)\n",
    "RETURN id(c) as cid, c.file_name as docfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb2709",
   "metadata": {},
   "source": [
    "### Build a CSV of edges linking chunks to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p graphdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b6945-2d40-4bc5-be26-bc6683d86b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# write edges from chunks to document node\n",
    "\n",
    "with open('graphdata/chunk2doc.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['~id','~from','~to','~label'])\n",
    "\n",
    "    for res in chunks_per_file['results']:\n",
    "        chunk_node_id = res['cid']\n",
    "        docfile = res['docfile']\n",
    "        docid = docfile.split(\".\")[0]\n",
    "        writer.writerow([f\"ce_{chunk_node_id}\", chunk_node_id, docid, \"belongsToDoc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d4e51-1086-41b2-8a17-08a81fbde126",
   "metadata": {},
   "source": [
    "### Copy graphdata files to S3 so we can load to Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_WORKING_BUCKET=\"<your working bucket - without leading s3:// or trailing slash >\"\n",
    "S3_SOURCE=f\"s3://{S3_WORKING_BUCKET}/chunk2doc.csv\"\n",
    "S3_SOURCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e632a-6f0d-4746-8147-0f871a8b571b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$S3_SOURCE\"\n",
    "\n",
    "aws s3 cp graphdata/chunk2doc.csv $1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43e96f",
   "metadata": {},
   "source": [
    "### Batch-load to Neptune graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b3ccc-e08c-4ae8-bcfa-53f85179a881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%oc\n",
    "\n",
    "CALL neptune.load({\n",
    "    format: \"csv\", \n",
    "    source: \"${S3_SOURCE}\", \n",
    "    region : \"${region}\",\n",
    "    format: \"csv\",\n",
    "    failOnError: False,\n",
    "    concurrency: 1\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516536fe",
   "metadata": {},
   "source": [
    "### Verify using a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fcb62-c301-4546-a3c3-1575beee3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc\n",
    "\n",
    "MATCH (d:DOCUMENT)<-[:belongsToDoc]-(c:Chunk)\n",
    "RETURN d.title, collect(id(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdee1d",
   "metadata": {},
   "source": [
    "### Finally we can combine vector similarity with observations from Comprehend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embed_model.get_text_embedding(\"career skills\")\n",
    "embparams={'emb': embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297eec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc -qp embparams\n",
    "\n",
    "WITH $emb as emb\n",
    "CALL neptune.algo.vectors.topKByEmbedding(emb)\n",
    "YIELD embedding, node, score\n",
    "\n",
    "MATCH path=(node:Chunk)-[:belongsToDoc]->(d:DOCUMENT)-[ev]->(obs)-[role]->(ent)\n",
    "RETURN path\n",
    "LIMIT 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
