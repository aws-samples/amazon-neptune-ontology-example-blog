{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2d9b7-5ab8-4f26-b3b0-bd0c4786cf39",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# KGC 2024 MasterClass: Generating and analyzing knowledge graphs using GenAI and Neptune Analytics\n",
    "# Notebook 0: Prep Data\n",
    "\n",
    "This notebook prepares organizational graph data for the demo accompanying the master class. \n",
    "\n",
    "You can skip this if you like. The results of the data prep are shared in a public S3 bucket. You still may find it useful to explore *how* the data was prepared. \n",
    "\n",
    "Our model for this masterclass is the following:\n",
    "\n",
    "<img src=\"images/kgc_model.png\">\n",
    "\n",
    "In this notebook, we prepare three sets of data.\n",
    "\n",
    "\n",
    "1. Base org data: organizations, persons, and industries. These are the blue boxes in the figure above.\n",
    "\n",
    "2. Press release documents and their text content. This is the white box in the figure above.\n",
    "\n",
    "3. Amazon Comprehend extraction of press releases in graph form. We link the extraction results to the base org data.  These are the yellow boxes in the figure above.\n",
    "\n",
    "The next figure depicts our design.\n",
    "\n",
    "<img src=\"images/kgc_design.png\">\n",
    "\n",
    "In this notebook we prep structured data and extracted entities from unstructured data in the bottom third of the figure. \n",
    "\n",
    "Run this notebook in a notebook instance with access to a public S3 bucket. It creates its results in the file system of the notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18867a0",
   "metadata": {},
   "source": [
    "## Get source data\n",
    "We require source data.\n",
    "- dbpedia_orgs.csv - Organizations\n",
    "- comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl - Press releases\n",
    "- er_curated.csv - Resolved organizations\n",
    "\n",
    "Get from public S3 bucket into local source folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267563be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_notebook as gn\n",
    "config = gn.configuration.get_config.get_config()\n",
    "\n",
    "region = config.aws_region\n",
    "s3_bucket = f\"s3://aws-neptune-customer-samples-{region}/kgc2024_na/source/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$s3_bucket\"\n",
    "\n",
    "aws s3 sync $1 source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed07fa",
   "metadata": {},
   "source": [
    "## Create prep output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p graphdata rawtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ddf27-8724-428e-aff7-e7a472d9cdc7",
   "metadata": {},
   "source": [
    "## Create base org, person, and industry data\n",
    "\n",
    "We will add a few orgs, industries, and persons to our graph as base KG data. The result is a set of CSV files written to the graphdata folder:\n",
    "\n",
    "- seed_orgs.csv - Organizations\n",
    "- seed_persons.csv - Persons belonging to orgs\n",
    "- seed_industries.csv - Industries of organizations.\n",
    "- seed_rels.csv - Org relationships to persons, industries, and other orgs.\n",
    "\n",
    "We'll use DBPedia as a source.\n",
    "\n",
    "To get a few of the orgs, run the following query on https://dbpedia.org/sparql against default named graph http://dbpedia.org\n",
    "\n",
    "```\n",
    "select * where \n",
    "{\n",
    " values ?company { \n",
    "<http://dbpedia.org/resource/Rivian> \n",
    " <http://dbpedia.org/resource/Whole_Foods_Market>\n",
    "<http://dbpedia.org/resource/Amazon_(company)>\n",
    "<http://dbpedia.org/resource/Amazon_Web_Services>\n",
    "<http://dbpedia.org/resource/Lockheed_Martin>\n",
    " } .\n",
    "OPTIONAL { ?company dbo:type ?otype . } .\n",
    "OPTIONAL { ?company dbp:currentStatus ?pstatus . } .\n",
    "OPTIONAL { ?company dbp:industry ?pindustry . } .\n",
    "OPTIONAL { ?company dbo:keyPerson ?okeyPerson . } .\n",
    "OPTIONAL { ?company dbp:name ?pname . } .\n",
    "OPTIONAL { ?company dbp:parent ?pparent . } .\n",
    "OPTIONAL { ?company dbp:type ?ptype . } .\n",
    "OPTIONAL { ?company dbp:url ?purl . } .\n",
    "OPTIONAL { ?company foaf:homepage ?fhomepage . } .\n",
    "OPTIONAL { ?company foaf:name ?fname . } .\n",
    "\n",
    "} \n",
    "ORDER BY ?company\n",
    "```\n",
    "\n",
    "To get some of these people, run the following query:\n",
    "\n",
    "```\n",
    "select * where \n",
    "{\n",
    " values ?person { \n",
    "<http://dbpedia.org/resource/Andy_Jassy> \n",
    "<http://dbpedia.org/resource/Jeff_Bezos> \n",
    "<http://dbpedia.org/resource/James_D._Taiclet>\n",
    " } .\n",
    "OPTIONAL { ?person foaf:name ?fname . } .\n",
    "\n",
    "} \n",
    "ORDER BY ?person\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030f5e3-86eb-43cb-8b56-2e2b4212025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# return cell value or empty string\n",
    "def cell_val(dicto, key):\n",
    "    if key in dicto:\n",
    "        return dicto[key]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "# make delimited list of vals for a cell\n",
    "def multi_val(dicto, key):\n",
    "    if key in dicto:\n",
    "        return \";\".join(dicto[key])\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "# make an ID, if val is already an IRI, return it, else build one\n",
    "def as_id(val, objtype):\n",
    "    if val.startswith(\"http://\"):\n",
    "        return val\n",
    "    else:\n",
    "        return f\"{objtype}_{val}\"\n",
    "\n",
    "# boolean: is val an IRI?\n",
    "def is_iri(val):\n",
    "    return val.startswith(\"http://\")\n",
    "\n",
    "# Known persons\n",
    "PERSONS={\n",
    "    \"http://dbpedia.org/resource/Andy_Jassy\": \"Andy Jassy\",\n",
    "    \"http://dbpedia.org/resource/James_D._Taiclet\": \"James D. Taiclet\",\n",
    "    \"http://dbpedia.org/resource/Jeff_Bezos\": \"Jeff Bezos\"\n",
    "}\n",
    "\n",
    "# tracker data structures\n",
    "orgs = {}\n",
    "o2p = {}\n",
    "o2i = {}\n",
    "o2o = {}\n",
    "industries={}\n",
    "\n",
    "# build orgs dynamically; loop through results from DBPedia\n",
    "df = pd.read_csv(filepath_or_buffer=\"source/dbpedia_orgs.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # Company ID, which is an IRI, maps to org record in orgs dictionary\n",
    "    company_id = row['company']\n",
    "    org = {}\n",
    "    if not (company_id in orgs):\n",
    "        orgs[company_id] = org\n",
    "    else:\n",
    "        org = orgs[company_id]\n",
    "\n",
    "    # If val is defined, add to org record\n",
    "    def add_single(key, val):\n",
    "        if str(val) == \"nan\":\n",
    "            return\n",
    "        org[key] = val\n",
    "        \n",
    "    # If val is defined, add to org multi-val record\n",
    "    def add_multi(key, val):\n",
    "        if str(val) == \"nan\":\n",
    "            return\n",
    "        if not key in org:\n",
    "            org[key] = [val]\n",
    "        elif not(val in org[key]):\n",
    "            org[key].append(val)\n",
    "        \n",
    "    add_single('leType', row['otype'])\n",
    "    add_single('leStatus', row['pstatus'])\n",
    "    add_single('parent', row['pparent'])\n",
    "    add_multi('industries', row['pindustry'])\n",
    "    add_multi('names', row['pname'])\n",
    "    add_multi('names', row['fname'])\n",
    "    if row['okeyPerson'] in PERSONS:\n",
    "        add_multi('persons', row['okeyPerson'])\n",
    "\n",
    "    # track distinct industry nodes\n",
    "    # and add Org-Industry edges\n",
    "    if 'industries' in org:\n",
    "        for i in org['industries']:\n",
    "            industry_id = as_id(i, \"industry\")\n",
    "            industries[industry_id] = 'dontcare' # need it to build industry nodes\n",
    "            edge_id = f\"eo2i_{company_id}_{industry_id}\"\n",
    "            if not (edge_id in o2i):\n",
    "                o2i[edge_id] = [edge_id, company_id, industry_id, \"hasIndustry\"]\n",
    "    # Add Org-Person edges\n",
    "    if 'persons' in org:\n",
    "        for p in org['persons']:\n",
    "            edge_id = f\"eo2p_{company_id}_{p}\"\n",
    "            if not (edge_id in o2i):\n",
    "                o2p[edge_id] = [edge_id, company_id, p, \"hasKnownPerson\"]\n",
    "    # Add Org-Org edges\n",
    "    if 'parent' in org and is_iri(org['parent']):\n",
    "        edge_id = f\"eo2o_{company_id}_{org['parent']}\"\n",
    "        if not (edge_id in o2o):\n",
    "            o2p[edge_id] = [edge_id, company_id, org['parent'], \"hasParentCompany\"]\n",
    " \n",
    "# write persons to CSV\n",
    "with open('graphdata/seed_persons.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"name\"])\n",
    "    for p in PERSONS:\n",
    "        writer.writerow([p, \"OrgKG_Person\", PERSONS[p]])\n",
    "\n",
    "# write orgs to CSV\n",
    "with open('graphdata/seed_orgs.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"names\", \"leType\", \"leStatus\"])\n",
    "    for o in orgs:\n",
    "        writer.writerow([o, \"OrgKG_Organization\", multi_val(orgs[o], 'names'), cell_val(orgs[o], 'leType'), cell_val(orgs[o], 'leStatus')])\n",
    "\n",
    "# write industries to CSV\n",
    "with open('graphdata/seed_industries.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\"])\n",
    "    for i in industries:\n",
    "        writer.writerow([i, \"OrgKG_Industry\"])\n",
    "\n",
    "# write edges to CSV\n",
    "with open('graphdata/seed_rels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\",\"~from\", \"~to\", \"~label\"])\n",
    "    for e in o2p:\n",
    "        writer.writerow(o2p[e])\n",
    "    for e in o2i:\n",
    "        writer.writerow(o2i[e])\n",
    "    for e in o2o:\n",
    "        writer.writerow(o2o[e])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf1f3e-7db2-4c0f-82f9-cb71d1c1fa81",
   "metadata": {},
   "source": [
    "## Build press release documents\n",
    "\n",
    "We have a set of press releases. These documents contain useful information that we would like to link to our base organization KG. \n",
    "\n",
    "In this set we build those documents.\n",
    "\n",
    "The result is a CSV file written to the *graphdata* folder:\n",
    "\n",
    "- prdocs.csv \n",
    "\n",
    "Also many *.txt* files are written to the *raw_text* folder. The name of each file is *<docid>.txt, where *docid* is the vertex ID of the document in the graph.\n",
    "    \n",
    "We do not load the text intent the graph ... not yet, anyway. Later (in 2-CreateLlamaIndex.ipynb) we will show how to use LlamaIndex with Neptune. There will be add the text and embeddings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887b1e4-bbab-4a56-8330-2e95940f239e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prdocs = []\n",
    "\n",
    "# Open the JSONL file that contains the documents plus the Comprehend extraction. \n",
    "jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "for index, row in jsonObj.iterrows():\n",
    "        \n",
    "    # extract metadata about current press release\n",
    "    metadata=row['metadata']\n",
    "    m_keywords=metadata['keywords']\n",
    "    m_title=metadata['title']\n",
    "    m_doc=metadata['document_id']\n",
    "\n",
    "    # write text to a file for chunking/embedding later\n",
    "    with open(f\"rawtext/{m_doc}.txt\", \"w\") as f:\n",
    "        f.write(row['raw_text'])\n",
    "    \n",
    "    prdocs.append([m_doc, \"DOCUMENT\", m_title, m_keywords])\n",
    "\n",
    "# write docs to CSV\n",
    "with open('graphdata/prdocs.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"title\", \"keywords\"])\n",
    "    for p in prdocs:\n",
    "        writer.writerow(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5711c-af4f-466f-90dd-e88a42ae0f07",
   "metadata": {},
   "source": [
    "## Build Comprehend extraction results\n",
    "\n",
    "We ran Amazon Comprehend to extract entities and events from the press releases. \n",
    "\n",
    "In this step, we build those extractions and link them to documents and organizations.\n",
    "\n",
    "The results is a set of CSV files written to the *graphdata* folder:\n",
    "\n",
    "- comprehend_nodes.csv\n",
    "- comprehend_edges.csv\n",
    "\n",
    "For more on this code, see blog post https://aws.amazon.com/blogs/database/building-a-knowledge-graph-in-amazon-neptune-using-amazon-comprehend-events/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c94e4c-3268-4045-90af-e65e929f8960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprehend vertices\n",
    "class Vertex:\n",
    "    def __init__(self):\n",
    "        self.words = set()\n",
    "        self.primaryName = \"\"\n",
    "        self.entityType = \"\"\n",
    "        self.id = \"\"\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        self.words.add(word)\n",
    "    \n",
    "    def setPrimaryName(self, name):\n",
    "        self.primaryName = name\n",
    "        \n",
    "    def setEntityType(self, entityType):\n",
    "        self.entityType = entityType\n",
    "        \n",
    "    def setId(self, rawIdString):\n",
    "        self.id = \"_\".join(rawIdString.split()).lower() # replace all whitespace with underscores\n",
    "\n",
    "    def getId(self):\n",
    "        return self.id\n",
    "    \n",
    "    def toString(self):\n",
    "        print(\"Entity \" + self.getId() + \"; type=\" + self.entityType + \"; words: \" + str(self.words))\n",
    "        \n",
    "# Comprehend edges\n",
    "class Edge:\n",
    "    def __init__(self, fromEntity, toEntity, edgeType):\n",
    "        self.fromEntity = fromEntity\n",
    "        self.toEntity = toEntity\n",
    "        self.edgeType = edgeType\n",
    "    \n",
    "    def getId(self):\n",
    "        return \"_\".join((\"edge__\" + self.fromEntity.getId() + \"_\" + self.toEntity.getId() + \"_\" + self.edgeType).split()).lower()  # replace all whitespace with underscores\n",
    "    \n",
    "    def toString(self):\n",
    "        print(\"Edge \" + self.fromEntity.getId() + \" --\" + self.edgeType + \"-> \" + self.toEntity.getId())\n",
    "\n",
    "nodeList = []\n",
    "edgeList = []\n",
    "nodeWordList = {}\n",
    "# We will filter out names referring to each entity with less than 0.95 group certainty.\n",
    "# You can change this threshold to be lower if you are tolerant of less certain values in your data set.\n",
    "groupThreshold = 0.95\n",
    "\n",
    "# Open the JSONL file that contains the documents plus the Comprehend extraction. \n",
    "jsonObj = pd.read_json(path_or_buf=\"source/comprehend_events_amazon_press_releases.20201118.v1.4.1.jsonl\", lines=True)\n",
    "for index, row in jsonObj.iterrows():\n",
    "        \n",
    "    # extract metadata about current press release\n",
    "    metadata=row['metadata']\n",
    "    document_id=metadata['document_id']\n",
    "    documentNode = Vertex()\n",
    "    documentNode.setId(document_id)\n",
    "    \n",
    "    # Comprehend Events references entities it refers to by index, so we need to retain the ordered list of entities\n",
    "    # within the document\n",
    "    docEntityList = []\n",
    "    annotations = row['annotations']\n",
    "    for entity in annotations[\"Entities\"]:\n",
    "        # convert each object under the \"Entities\" list into a Node\n",
    "        theEntity = Vertex()\n",
    "        theEntity.setPrimaryName(entity[\"Mentions\"][0][\"Text\"])\n",
    "        theEntity.setEntityType(entity[\"Mentions\"][0][\"Type\"])\n",
    "        theEntity.setId(\"node__\" + entity[\"Mentions\"][0][\"Type\"] + \"_\" + entity[\"Mentions\"][0][\"Text\"])\n",
    "        for mention in entity[\"Mentions\"]:\n",
    "            if (mention[\"GroupScore\"] >= groupThreshold):\n",
    "                theEntity.addWord(mention[\"Text\"])\n",
    "\n",
    "        docEntityList.append(theEntity)\n",
    "        nodeList.append(theEntity)\n",
    "        \n",
    "    for event in annotations[\"Events\"]:\n",
    "        #convert each object under the \"Events\" list to a Node\n",
    "        theEntity = Vertex()\n",
    "        theEntity.setEntityType(event[\"Type\"])\n",
    "        theEntity.setPrimaryName(event[\"Triggers\"][0][\"Text\"])\n",
    "        theEntity.setId(\"node__event_\" + document_id + \"_\" + event[\"Type\"] + \"_\" + event[\"Triggers\"][0][\"Text\"] + str(event[\"Triggers\"][0][\"BeginOffset\"]))\n",
    "        for trigger in event[\"Triggers\"]:\n",
    "            theEntity.addWord(trigger[\"Text\"])\n",
    "\n",
    "        nodeList.append(theEntity)\n",
    "\n",
    "        # add edges between the event node and the entity node, \n",
    "        # annotated with a label describing the Comprehend Event role assigned to the entity in the event.\n",
    "        for argument in event[\"Arguments\"]:\n",
    "            edgeList.append(Edge(theEntity, docEntityList[argument[\"EntityIndex\"]], argument[\"Role\"]))\n",
    "        \n",
    "        # add an edge between the document and the event nodes\n",
    "        edgeList.append(Edge(documentNode, theEntity, \"EVENT\"))\n",
    "        \n",
    "# write all of our nodes to a CSV file\n",
    "with open('graphdata/comprehend_nodes.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['~id','~label','primaryName','names'])\n",
    "    for node in nodeList:\n",
    "        for word in node.words:\n",
    "            # there will be a row for each word assigned to the entity, \n",
    "            # but Neptune will aggregate them into single set of words on the node\n",
    "            writer.writerow([node.id, node.entityType, node.primaryName, word])\n",
    "\n",
    "# write all of our nodes to a CSV file\n",
    "with open('graphdata/comprehend_edges.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['~id','~from','~to','~label'])\n",
    "    for edge in edgeList:\n",
    "        writer.writerow([edge.getId(), edge.fromEntity.id, edge.toEntity.id, edge.edgeType])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de59bf-aaeb-4997-8c09-2fece789b14d",
   "metadata": {},
   "source": [
    "## Link comprehend orgs to base KG orgs\n",
    "\n",
    "Press releases mention organizations like Amazon and Whole Foods. There are many possible variations. One press release could use the name *amazon.com*, another *Amazon*. We would like to link them the base organization from our KG. If we can do that, then the press release becomes useful structured data that enhances our understanding of organiations we have already baselines in the graph.\n",
    "\n",
    "There is lots of noise too. For example, Comprehend shows *agencies* and *developers* as extracted organizations. We wish to ignore *common nouns* and consider only *proper nouns*. \n",
    "\n",
    "In this step we do two things:\n",
    "1. Create orgs for any orgs mentioned in press releases that are NOT in our base orgs.\n",
    "2. Link each org mentioned in the press release to a base org. This ends up being a quick/dirty entity resolution exercise. \n",
    "\n",
    "The result is a set of CSV files in *graphdata* folder:\n",
    "\n",
    "- discovered_orgs.csv - Extraced orgs promoted to base orgs\n",
    "- resolved_org_rels.csv - Edges connecting extracted orgs to base org\n",
    "\n",
    "Note: we hand-curated these files but discuss in the class design approaches to resolve these entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7baaf5-6d71-4492-b81b-224e8ae5e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get curated list extracted primary org names and how to resolve them\n",
    "discovered_orgs=[]\n",
    "org_name_to_id={}\n",
    "df = pd.read_csv(filepath_or_buffer=\"source/er_curated.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    org_name = row['Org']\n",
    "    discovered = row['Discovered']\n",
    "    resolved = row['Resolved']\n",
    "    if discovered == 'Y':\n",
    "        org_id = as_id(org_name, \"resolved_org\")\n",
    "        discovered_orgs.append([org_id, \"OrgKG_Organization\", org_name])\n",
    "        org_name_to_id[org_name] = org_id\n",
    "    elif str(resolved) != \"nan\":\n",
    "        org_name_to_id[org_name] = resolved\n",
    "\n",
    "# Link observed orgs to base orgs\n",
    "resolved_orgs={}\n",
    "df = pd.read_csv(filepath_or_buffer=\"graphdata/comprehend_nodes.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    if row['~label']=='ORGANIZATION':\n",
    "        prname=row['primaryName']\n",
    "        if prname in org_name_to_id:\n",
    "            org_id=row[\"~id\"]\n",
    "            resolved_id=org_name_to_id[prname]\n",
    "            edge_id = f\"ereso_{org_id}_{resolved_id}\"\n",
    "            if not (edge_id in resolved_orgs):\n",
    "                resolved_orgs[edge_id] = [edge_id, org_id, resolved_id, \"resolvesToOrg\"]\n",
    "            \n",
    "\n",
    "# write discovered orgs to CSV\n",
    "with open('graphdata/discovered_orgs.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\", \"~label\", \"name\"])\n",
    "    for o in discovered_orgs:\n",
    "        writer.writerow(o)\n",
    "\n",
    "# write edges to CSV\n",
    "with open('graphdata/resolved_org_rels.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"~id\",\"~from\", \"~to\", \"~label\"])\n",
    "    for e in resolved_orgs:\n",
    "        writer.writerow(resolved_orgs[e])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
