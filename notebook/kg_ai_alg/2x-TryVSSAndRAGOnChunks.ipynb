{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f2d9b7-5ab8-4f26-b3b0-bd0c4786cf39",
   "metadata": {},
   "source": [
    "# Graph RAG Pattern: Vector + KG using VSS\n",
    "\n",
    "This supplements notebook 2, where we used LlamaIndex to create a vector store of press releases and linked them to document nodes. In this notebook we demonstrate a graph RAG pattern that performs a hybrid vector/traversal query. It starts by finding similar embeddings. Then it traverses outward to find related events and organizations. \n",
    "\n",
    "Here is our data model.\n",
    "\n",
    "<img src=\"images/kgc_model.png\">\n",
    "\n",
    "The next figure depicts our design.\n",
    "\n",
    "<img src=\"images/kgc_design.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d76f6c-3767-49d3-acc9-837356829281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -q llama-index llama-index-vector-stores-neptune llama-index-graph-stores-neptune  llama-index-llms-bedrock llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182a051-2a2f-42cb-8807-748cf94276e6",
   "metadata": {},
   "source": [
    "### Get the Graph Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_notebook as gn\n",
    "config = gn.configuration.get_config.get_config()\n",
    "\n",
    "region = config.aws_region\n",
    "graph_identifier=config._host.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763d739-cfd3-483c-9f06-94168e2da2cb",
   "metadata": {},
   "source": [
    "### Imports and Global Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce64488-0f3c-401e-a93b-1bac23f567fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, KnowledgeGraphIndex, Settings, load_index_from_storage\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "QA_MODEL=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# define LLM\n",
    "llm = Bedrock(model=QA_MODEL, \n",
    "    model_kwargs={\"temperature\": 0})\n",
    "embed_model = BedrockEmbedding(model=\"amazon.titan-embed-text-v1\")\n",
    "\n",
    "# Set global LLM settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4bd21",
   "metadata": {},
   "source": [
    "### Look at the chunks in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc\n",
    "\n",
    "MATCH (n:Chunk) \n",
    "CALL neptune.algo.vectors.get(n)\n",
    "YIELD embedding\n",
    "RETURN n.file_name as docfile, id(n) as chunkid, n.text as text, embedding\n",
    "ORDER BY docfile\n",
    "LIMIT 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc\n",
    "\n",
    "MATCH (d:DOCUMENT)<-[:belongsToDoc]-(c:Chunk)\n",
    "RETURN id(d) as docid, d.title as title, collect(id(c)) as chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a984617",
   "metadata": {},
   "source": [
    "## Try Vector Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbcdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Does Amazon have a fulfillment center in Mississippi?\"\n",
    "embedding = embed_model.get_text_embedding(query)\n",
    "embparams={'emb': embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297eec8d",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%oc -qp embparams --store-to qres\n",
    "\n",
    "WITH $emb as emb\n",
    "CALL neptune.algo.vectors.topKByEmbedding(emb)\n",
    "YIELD embedding, node, score\n",
    "\n",
    "// FILTER by good score\n",
    "WHERE score < 560.0\n",
    "WITH node, id(node) as chunkid\n",
    "\n",
    "MATCH  path=(node:Chunk)-[:belongsToDoc]->(d:DOCUMENT)-[ev]->(obs)-[role]->(ent)-[:resolvesToOrg *]->(org)-[:hasKnownPerson|hasParentCompany|hasIndustry *]->(orgrel)\n",
    "RETURN path\n",
    "LIMIT 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc -qp embparams --store-to qtext\n",
    "\n",
    "WITH $emb as emb\n",
    "CALL neptune.algo.vectors.topKByEmbedding(emb)\n",
    "YIELD embedding, node, score\n",
    "\n",
    "// FILTER by good score\n",
    "WHERE score < 560.0\n",
    "WITH node, id(node) as chunkid\n",
    "\n",
    "MATCH  (node:Chunk)-[:belongsToDoc]->(d:DOCUMENT)\n",
    "\n",
    "RETURN id(node) as chunk_id, node.text as chunk_text, id(d) as doc_id, d.title as doc_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%oc -qp embparams --store-to qres\n",
    "\n",
    "WITH $emb as emb\n",
    "CALL neptune.algo.vectors.topKByEmbedding(emb)\n",
    "YIELD embedding, node, score\n",
    "\n",
    "// FILTER by good score\n",
    "WHERE score < 560.0\n",
    "WITH node, id(node) as chunkid\n",
    "\n",
    "MATCH  (node:Chunk)-[:belongsToDoc]->(d:DOCUMENT)-[ev]->(obs)-[role]->(ent)-[:resolvesToOrg *]->(org)\n",
    "OPTIONAL MATCH (org)-[orgev:hasKnownPerson|hasIndustry]->(orgrel)\n",
    "\n",
    "RETURN distinct id(d) as doc_id, d.title as doc_title, \n",
    "    labels(obs) as event, obs.primaryName as event_names,\n",
    "    type(role) as role_type,\n",
    "    labels(ent) as entity, collect(distinct ent.names) as entity_names,\n",
    "    id(org) as org,\n",
    "    collect(distinct id(orgrel)) as org_relationships\n",
    "    \n",
    "LIMIT 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "SYSTEM_PROMPT=\"You are an expert Q&A system that is trusted around the world.\\\\nAlways answer the query using the provided context information, and not prior knowledge.\\\\nSome rules to follow:\\\\n1. Never directly reference the given context in your answer.\\\\n2. Avoid statements like \\'Based on the context, ...\\' or \\'The context information ...\\' or anything along those lines.\"\n",
    "\n",
    "def make_prompt(q, pcontext):\n",
    "    return \"\\n\".join( [\n",
    "        \"Context information is below.\",\n",
    "        \"---------------------\",\n",
    "        pcontext,\n",
    "        \"Given the context information and not prior knowledge, answer the query.\",\n",
    "        f\"Query: {q}\",\n",
    "        \"Answer: \"\n",
    "    ])\n",
    "\n",
    "def get_completion(prompt):\n",
    "    print(prompt)\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": '',\n",
    "            \"max_tokens\": 2000,\n",
    "            \"messages\":[{\"role\":\"user\", \"content\":[{\"text\": prompt, \"type\": \"text\"}]}],\n",
    "            \"temperature\": 0.1,\n",
    "            \"system\": SYSTEM_PROMPT\n",
    "        }\n",
    "    )    \n",
    "    \n",
    "    response = bedrock_client.invoke_model(body=body, modelId=QA_MODEL)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    return response_body.get('content')[0].get('text')\n",
    "    \n",
    "get_completion(make_prompt(\n",
    "    query, \n",
    "    str({'textresult': qtext, 'pathresult': qres})\n",
    "))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
